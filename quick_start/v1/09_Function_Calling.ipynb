{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "Function calling in OpenAI allows you to integrate AI-generated text with external functions. \\\n",
    "This enables you to execute code, perform calculations, or fetch data based on the model's output. \\\n",
    "By defining functions and allowing the model to call them, you can create more interactive and dynamic applications.\n",
    "\n",
    "For more detailed examples, refer to the official documentation [Function Calling with Azure OpenAI Service](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/function-calling?tabs=non-streaming%2Cpython)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), \n",
    "  api_key=os.getenv(\"AZURE_OPENAI_KEY\"),  \n",
    "  api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "CHAT_COMPLETIONS_MODEL = os.getenv('CHAT_COMPLETION_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions that will take actions like retrieve information from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": unit})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": unit})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": unit})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_VUwpgt7KLVLcFrkUV394ojmS', function=Function(arguments='{\\n    \"location\": \"San Francisco\",\\n    \"unit\": \"fahrenheit\"\\n}', name='get_current_weather'), type='function')])\n",
      "[ChatCompletionMessageToolCall(id='call_VUwpgt7KLVLcFrkUV394ojmS', function=Function(arguments='{\\n    \"location\": \"San Francisco\",\\n    \"unit\": \"fahrenheit\"\\n}', name='get_current_weather'), type='function')]\n",
      "{\n",
      "    \"location\": \"San Francisco\",\n",
      "    \"unit\": \"fahrenheit\"\n",
      "}\n",
      "ChatCompletion(id='chatcmpl-9f0IFQCXDTsvAzeVtxyCRXfwwiZSp', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The current temperature in San Francisco is 72 degrees Fahrenheit.', role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1719559603, model='gpt-35-turbo', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=12, prompt_tokens=180, total_tokens=192), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
     ]
    }
   ],
   "source": [
    "def run_conversation():\n",
    "    # Step 1: send the conversation and available functions to the model\n",
    "    messages = [{\"role\":\"system\", \"content\":\"\"\"You are an AI assistant designed to help users get information about the weather. \n",
    "                 When a user asks for information about the weather, you should use the tool function named 'get_current_weather'. \n",
    "                 Only use the functions you have been provided with.\n",
    "                Don't make assumptions about what values to plug into functions. \n",
    "                 Ask for clarification if a user request is ambiguous.\n",
    "                 The format of the arguments should not contain '/*' and be a valid JSON string with property names enclosed in double quotes.\"\"\"},\n",
    "                 {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"get_current_weather\",\n",
    "                \"description\": \"Get the current weather in a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                        },\n",
    "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                    },\n",
    "                    \"required\": [\"location\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=CHAT_COMPLETIONS_MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice = {\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\"}}\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    # Step 2: check if the model wanted to call a function\n",
    "    if tool_calls:\n",
    "        # Step 3: call the function\n",
    "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "        available_functions = {\n",
    "            \"get_current_weather\": get_current_weather,\n",
    "        }  # only one function in this example, but you can have multiple\n",
    "        messages.append(response_message)  # extend conversation with assistant's reply\n",
    "        # Step 4: send the info for each function call and function response to the model\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            # Sometimes the name is classified mistakenly as python\n",
    "            if function_name == \"python\":\n",
    "                function_name = \"get_current_weather\"\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = function_to_call(\n",
    "                location=function_args.get(\"location\"),\n",
    "                unit=function_args.get(\"unit\"),\n",
    "            )\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )  # extend conversation with function response\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=CHAT_COMPLETIONS_MODEL,\n",
    "            messages=messages,\n",
    "        )  # get a new response from the model where it can see the function response\n",
    "        return second_response\n",
    "print(run_conversation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use cases to explore and think about with the customer\n",
    "1. **Dynamic Calculations**  \\\n",
    "Perform real-time calculations based on user input.\n",
    "\n",
    "2. **Data Fetching** \\\n",
    "Retrieve data from external APIs based on the model's output.\n",
    "\n",
    "3. **Interactive Applications** \\\n",
    "Create more interactive and dynamic applications by integrating AI with backend functions.\n",
    "\n",
    "#### Best Practices\n",
    "1. **Clear Prompts** \\\n",
    "Ensure prompts are clear and specific to get accurate responses from the model.\n",
    "\n",
    "2. **Robust Parsing** \\\n",
    "Implement robust parsing logic to handle various formats of model output.\n",
    "\n",
    "3. **Error Handling** \\\n",
    "Include error handling to manage unexpected outputs or function call failures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
