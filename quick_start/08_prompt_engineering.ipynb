{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\",\"\").strip()\n",
    "assert API_KEY, \"ERROR: Azure OpenAI Key is missing\"\n",
    "openai.api_key = API_KEY\n",
    "\n",
    "RESOURCE_ENDPOINT = os.getenv(\"OPENAI_API_BASE\",\"\").strip()\n",
    "assert RESOURCE_ENDPOINT, \"ERROR: Azure OpenAI Endpoint is missing\"\n",
    "assert \"openai.azure.com\" in RESOURCE_ENDPOINT.lower(), \"ERROR: Azure OpenAI Endpoint should be in the form: \\n\\n\\t<your unique endpoint identifier>.openai.azure.com\"\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "\n",
    "\n",
    "# call OpenAI API with model name and prompt\n",
    "def call_openai_api(prompt, model_name=os.getenv('DEPLOYMENT_NAME'), max_token=400, stop=None, n=1, temperature=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "    engine=model_name,\n",
    "    messages = [{\"role\":\"system\", \"content\":\"You are a helpful assistant. \"},\n",
    "                {\"role\":\"user\",\"content\": prompt,}],\n",
    "        max_tokens=max_token,\n",
    "        stop=stop,\n",
    "        n=n,\n",
    "        temperature=temperature,)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought\n",
    "\n",
    "Experiment results demonstrate Zero-shot-CoT using single prompt template, significantly outperform zero-shot LLM performance on diverse benchmark reasoning tasks.  Without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002).\n",
    "\n",
    "Source: [Large Language Models are Zero-Shot Reasoners](https://arxiv.org/abs/2205.11916)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent GPT-35-Turbo has improved logic reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.\n"
     ]
    }
   ],
   "source": [
    "# This prompt gets wrong answer\n",
    "\n",
    "PROMPT_ZERO_SHOT = \"\"\"Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
    "and half of the golf balls are blue. How many blue golf balls are\n",
    "there?\n",
    "A: The answer (arabic numerals) is\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_ZERO_SHOT)\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### However, it still makes mistakes with one-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 blue golf balls.\n"
     ]
    }
   ],
   "source": [
    "# Still wrong answer with few-shot learning\n",
    "\n",
    "PROMPT_FEW_SHOT = \"\"\"Q: Roger has 5 tennis balss. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Roger have now?\n",
    "A: The answer is 11.\n",
    "\n",
    "Q: A juggler can juggle 16 balls. Half of the balls are golf balls and half of the golf balls are blue. How many blue golf balls are there?\n",
    "A:\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_FEW_SHOT)\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we need to find out how many golf balls there are in total. Since half of the balls are golf balls, we can divide the total number of balls by 2. So, 16 balls divided by 2 equals 8 golf balls.\n",
      "\n",
      "Next, we need to find out how many of these golf balls are blue. Since half of the golf balls are blue, we can again divide the number of golf balls by 2. So, 8 golf balls divided by 2 equals 4 blue golf balls.\n",
      "\n",
      "Therefore, there are 4 blue golf balls.\n"
     ]
    }
   ],
   "source": [
    "# With CoT, the answer is correct\n",
    "\n",
    "PROMPT_ZERO_SHOT_CoT = \"\"\"Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
    "and half of the golf balls are blue. How many blue golf balls are\n",
    "there?\n",
    "A: Letâ€™s think step by step.\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_ZERO_SHOT_CoT)\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If half of the balls are golf balls, then there are 16/2 = 8 golf balls.\n",
      "If half of the golf balls are blue, then there are 8/2 = 4 blue golf balls.\n",
      "Therefore, there are 4 blue golf balls.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROMPT_FEW_SHOT_CoT = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis\n",
    "balls. Each can has 3 tennis balls. How many tennis balls does\n",
    "he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6\n",
    "tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "Q: A juggler can juggle 16 balls. Half of the balls are golf balls,\n",
    "and half of the golf balls are blue. How many blue golf balls are\n",
    "there?\n",
    "A:\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_FEW_SHOT_CoT)\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More research on CoT prompt engineering\n",
    "\n",
    "\n",
    "<img src=\"assets/CoT.png\" alt=\"Alternative text\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Program-aided Language Models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The bakers baked 200 loaves of bread. They sold 93\n",
      "in the morning and 39 in the afternoon, which is a total\n",
      "of 132 loaves sold. They also had 6 unsold loaves\n",
      "returned. To find out how many loaves they have left,\n",
      "we subtract the total sold and returned from the\n",
      "initial amount baked: 200 - 132 - 6 = 62. The bakers\n",
      "have 62 loaves of bread left.\n"
     ]
    }
   ],
   "source": [
    "PROMPT_FEW_SHOT_CoT = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of\n",
    "tennis balls. Each can has 3 tennis balls. How many\n",
    "tennis balls does he have now?\n",
    "A: Roger started with 5 tennis balls. 2 cans of 3 tennis\n",
    "balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Q: The bakers at the Beverly Hills Bakery baked 200\n",
    "loaves of bread on Monday morning. They sold 93 loaves\n",
    "in the morning and 39 loaves in the afternoon. A grocery\n",
    "store returned 6 unsold loaves. How many loaves of\n",
    "bread did they have left?\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_FEW_SHOT_CoT)\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: The bakers baked 200 loaves of bread.\n",
      "  loaves_baked = 200\n",
      "They sold 93 loaves in the morning and 39 loaves in the afternoon.\n",
      "  loaves_sold = 93 + 39 = 132\n",
      "The grocery store returned 6 unsold loaves.\n",
      "  loaves_returned = 6\n",
      "The number of loaves left is\n",
      "  loaves_left = loaves_baked - loaves_sold + loaves_returned\n",
      "  loaves_left = 200 - 132 + 6 = 74\n"
     ]
    }
   ],
   "source": [
    "PROMPT_FEW_SHOT_PA = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of\n",
    "tennis balls. Each can has 3 tennis balls. How many\n",
    "tennis balls does he have now?\n",
    "A: Roger started with 5 tennis balls.\n",
    "  tennis_balls = 5\n",
    "2 cans of 3 tennis balls each is\n",
    "  bought_balls = 2 * 3 \n",
    "The answer is\n",
    "answer = tennis_balls + bought_balls\n",
    "tennis_balls_now = 5 + 6 = 11\n",
    "\n",
    "Q: The bakers at the Beverly Hills Bakery baked 200\n",
    "loaves of bread on Monday morning. They sold 93 loaves\n",
    "in the morning and 39 loaves in the afternoon. A grocery\n",
    "store returned 6 unsold loaves. How many loaves of bread\n",
    "did they have left?\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT_FEW_SHOT_PA)\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonsense Reasoning\n",
    "\n",
    "Paper: [Generated Knowledge Prompting for Commonsense Reasoning](https://arxiv.org/abs/2110.08387)\n",
    "\n",
    "\n",
    "\n",
    "Provide knowledge, turn knowledge question into reasoning. In general, more knowledge, better result.\n",
    "\n",
    "3 Contributing factors:\n",
    "\n",
    "(i) the quality of knowledge, \n",
    "\n",
    "(ii) the quantity of knowledge where the performance improves with more knowledge statements, and \n",
    "\n",
    "(iii) the strategy for integrating knowledge during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False. In golf, the objective is to have the lowest score possible. The player with the lowest score is the winner.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"The player with the lowest score wins.\n",
    "Is this true or false: Part of golf is trying to get a higher point total than others.\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT)\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An easel typically has three legs.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"A tripod is a kind of easel\n",
    "How many legs does an easel have?\n",
    "\"\"\"\n",
    "response = call_openai_api(PROMPT)\n",
    "\n",
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
