{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Azure OpenAI for Big Data\n",
        "\n",
        "The Azure OpenAI service can be used to solve a large number of natural language tasks through prompting the completion API. To make it easier to scale your prompting workflows from a few examples to large datasets of examples we have integrated the Azure OpenAI service with the distributed machine learning library [SynapseML](https://www.microsoft.com/en-us/research/blog/synapseml-a-simple-multilingual-and-massively-parallel-machine-learning-library/). This integration makes it easy to use the [Apache Spark](https://spark.apache.org/) distributed computing framework to process millions of prompts with the OpenAI service. This tutorial shows how to apply large language models at a distributed scale using Azure Open AI and Azure Synapse Analytics. \n",
        "\n",
        "## Step 1: Prerequisites\n",
        "\n",
        "The key prerequisites for this quickstart include a working Azure OpenAI resource, and an Apache Spark cluster with SynapseML installed. We suggest creating a Synapse workspace, but an Azure Databricks, HDInsight, or Spark on Kubernetes, or even a python environment with the `pyspark` package will work. \n",
        "\n",
        "1. An Azure OpenAI resource â€“ request access [here](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOFA5Qk1UWDRBMjg0WFhPMkIzTzhKQ1dWNyQlQCN0PWcu) before [creating a resource](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource)\n",
        "1. [Create a Synapse workspace](https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-create-workspace)\n",
        "1. [Create a serverless Apache Spark pool](https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-spark#create-a-serverless-apache-spark-pool)\n",
        "\n",
        "\n",
        "## Step 2: Import this guide as a notebook\n",
        "\n",
        "The next step is to add this code into your Spark cluster. You can either create a notebook in your Spark platform and copy the code into this notebook to run the demo. Or download the notebook and import it into Synapse Analytics\n",
        "\n",
        "1.\t[Download this demo as a notebook](https://github.com/microsoft/SynapseML/blob/master/notebooks/features/cognitive_services/CognitiveServices%20-%20OpenAI.ipynb) (click Raw, then save the file)\n",
        "1.\tImport the notebook [into the Synapse Workspace](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks#create-a-notebook) or if using Databricks [into the Databricks Workspace](https://docs.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage#create-a-notebook)\n",
        "1. Install SynapseML on your cluster. Please see the installation instructions for Synapse at the bottom of [the SynapseML website](https://microsoft.github.io/SynapseML/). Note that this requires pasting an additional cell at the top of the notebook you just imported\n",
        "3.\tConnect your notebook to a cluster and follow along, editing and rnnung the cells below.\n",
        "\n",
        "## Step 3: Fill in your service information\n",
        "\n",
        "Next, please edit the cell in the notebook to point to your service. In particular set the `service_name`, `deployment_name`, `location`, and `key` variables to match those for your OpenAI service:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from pyspark.sql.functions import  col, udf,mean,  concat, lit\n",
        "from pyspark.sql import functions as F\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from notebookutils import mssparkutils\n",
        "from notebookutils import mssparkutils\n",
        "\n",
        "\n",
        "dataset = load_dataset('opus_books','en-fr')\n",
        "df = dataset['train'].to_pandas().head(5000)\n",
        "# Convert to Spark dataframe\n",
        "df = spark.createDataFrame(df)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df = df.select('id', F.col('translation').getItem('fr').alias('fr'), F.col('translation')['en'].alias('en'))\n",
        "df = df.select(concat(lit(\"Translate this sentence to English: \"), col('fr')).alias('fr'), 'en')\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\n",
        "# Fill in the following lines with your service information\n",
        "deployment_name = \"text-ada-001\"\n",
        "key=mssparkutils.credentials.getSecret('mldemo4764731797' , 'OPENAI-API-KEY') # get OpenAI key from KeyVault\n",
        "key=mssparkutils.credentials.getSecret('mldemo4764731797' , 'OPENAI-API-ENDPOINT') # get OpenAI key from KeyVault"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create the OpenAICompletion Apache Spark Client\n",
        "\n",
        "To apply the OpenAI Completion service to your dataframe you just created, create an OpenAICompletion object which serves as a distributed client. Parameters of the service can be set either with a single value, or by a column of the dataframe with the appropriate setters on the `OpenAICompletion` object. Here we are setting `maxTokens` to 200. A token is around 4 characters, and this limit applies to the sum of the prompt and the result. We are also setting the `promptCol` parameter with the name of the prompt column in the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from synapse.ml.cognitive import OpenAICompletion\n",
        "\n",
        "completion = (\n",
        "    OpenAICompletion()\n",
        "    .setSubscriptionKey(key)\n",
        "    .setDeploymentName(deployment_name)\n",
        "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\n",
        "    .setMaxTokens(200)\n",
        "    .setPromptCol(\"fr\")\n",
        "    .setErrorCol(\"error\")\n",
        "    .setOutputCol(\"translated_en\")\n",
        ")\n",
        "\n",
        "# To process 5000 sentences, it takes 25 minutes using small cpu cluster\n",
        "# completed_df = completion.transform(df).cache()\n",
        "completed_df = completion.transform(df).select(\n",
        "        col(\"fr\"),\n",
        "        col(\"error\"),\n",
        "        col(\"translated_en.choices.text\").getItem(0).alias(\"text\"),\n",
        "        col('en')\n",
        ")\n",
        "completed_df = completed_df.select(col(\"fr\"), col(\"text\"), col(\"en\"),bleu(\"text\",\"en\").alias(\"bleu\")).cache()\n",
        "display(completed_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# TO DO: \n",
        "# Write code to measure the performance of the translation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
