{"file": "2201.11903.json", "summerization_type": "map_reduce", "summary": " Geva et al. (2021) explore the use of chain-of-thought prompting to improve the performance of large language models on multi-step reasoning tasks. Results show that chain-of-thought prompting can outperform the prior state of the art on StrategyQA and an unaided sports enthusiast on Sports Understanding. The paper also discusses ethical considerations, experiments conducted on various tasks, version control, reproducibility, computational resources, and examples of correct and incorrect chains of thought produced by LaMDA 137B and PaLM 540B."}