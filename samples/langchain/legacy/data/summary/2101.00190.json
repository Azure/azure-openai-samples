{"file": "2101.00190.json", "summerization_type": "map_reduce", "summary": " This paper proposes a lightweight alternative to fine-tuning for natural language generation tasks called prefix-tuning. This approach keeps the language model parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Results show that prefix-tuning outperforms fine-tuning in low-data settings and is more efficient, preserving the pretrained LM as much as possible. Additionally, it has the potential to scale to even larger models with a similar architecture."}