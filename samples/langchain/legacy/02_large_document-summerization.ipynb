{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Summerization\n",
    "\n",
    "Process:\n",
    "\n",
    "1. Generate summary for first page\n",
    "2. Refine summary with second page\n",
    "3. Repeate #2 for all pages\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Enviroments for following\n",
    "\n",
    "- Azure Form Recognizer Endpoint & Key\n",
    "- Azure OpenAI endpoint & key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\llms\\openai.py:1038: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from AzureOpenAIUtil.AzureFormRecognizer import AzureFormRecognizerRead\n",
    "from AzureOpenAIUtil.Summerization import summarization\n",
    "\n",
    "# File Directories \n",
    "# -- raw data\n",
    "RAW_DATA_FOLDER= 'data/raw'\n",
    "# -- extracted json file \n",
    "EXTRACTED_DATA_FOLDER = 'data/extracted'\n",
    "# -- summary files\n",
    "SUMMARY_FOLDER = 'data/summary'\n",
    "# Azure Form Recognizer Read\n",
    "azure_fr = AzureFormRecognizerRead()\n",
    "# Azure OpenAI util for summerization\n",
    "summary_eng = summarization(deployment_name=os.getenv('DEPLOYMENT_NAME'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run From Recognizer to OCR documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 2010.15980.pdf    write output to data/extracted\\2010.15980.json\n",
      "Processing file: 2101.00190.pdf    write output to data/extracted\\2101.00190.json\n",
      "Processing file: 2104.08691.pdf    write output to data/extracted\\2104.08691.json\n",
      "Processing file: 2110.08387.pdf    write output to data/extracted\\2110.08387.json\n",
      "Processing file: 2201.11903.pdf    write output to data/extracted\\2201.11903.json\n",
      "Processing file: 2203.11171.pdf    write output to data/extracted\\2203.11171.json\n",
      "Processing file: 2211.01910.pdf    write output to data/extracted\\2211.01910.json\n",
      "Processing file: 2212.10496.pdf    write output to data/extracted\\2212.10496.json\n"
     ]
    }
   ],
   "source": [
    "azure_fr.extract_files(RAW_DATA_FOLDER, EXTRACTED_DATA_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Summaries for extracted documents\n",
    "\n",
    "In this demo, we show how to generate one short summary for each document.\n",
    "\n",
    "*ignore the warning of input being truncated may show in the output - it's caused by GPT2 tokenizer for sequence longer than 1024*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Generate summary for file: 2010.15980.json\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "OpenAIChat currently only supports single prompt, got ['Write a concise summary of the following:\\n\\n\\n\"AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts Yasaman Razeghi *< Taylor Shin* Robert L. Logan IV* Eric Wallace Sameer Singh University of California, Irvine *University of California, Berkeley {tshin1, yrazeghi, rlogan, sameer } @uci.edu ericwallace@berkeley.edu Abstract The remarkable success of pretrained lan- guage models has motivated the study of what kinds of knowledge these models learn dur- ing pretraining. Reformulating tasks as fill- in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suit- able prompts. To address this, we develop AUTOPROMPT, an automated method to cre- ate prompts for a diverse set of tasks, based on a gradient-guided search. Using AUTO- PROMPT, we show that masked language mod- els (MLMs) have an inherent capability to perform sentiment analysis and natural lan- guage inference without additional parame- ters or finetuning, sometimes achieving per- formance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowl- edge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that au- tomatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a re- placement for finetuning. arXiv:2010.15980v2 [cs.CL] 7 Nov 2020 1 Introduction Pretrained language models (LMs) have had ex- ceptional success when adapted to downstream tasks via finetuning (Peters et al., 2018; Devlin et al., 2019). Although it is clear that pretrain- ing improves accuracy, it is difficult to determine whether the knowledge that finetuned LMs contain is learned during the pretraining or the finetuning * First three authors contributed equally. process. How can we directly evaluate the knowl- edge present in pretrained LMs, be it linguistic, factual, commonsense, or task-specific? Numerous techniques have been proposed to elicit such knowledge by analyzing pretrained LMs\\' internal representations. A common strategy is to use probing classifiers-shallow classifiers that predict certain attributes using an LMs\\' representa- tions as features (Conneau et al., 2018; Liu et al., 2019). However, probing classifiers require ad- ditional learned parameters and are thus suscep- tible to false positives; high probing accuracy is not a sufficient condition to conclude that an LM contains a certain piece of knowledge (Hewitt and Liang, 2019; Voita and Titov, 2020). Attention visualization, another common technique, has a similar failure mode: attention scores may be corre- lated with, but not caused by the underlying target knowledge, leading to criticism against their use as explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Both probing and attention visu- alizations also struggle to evaluate knowledge that cannot be represented as simple token- or sequence- level classification tasks. A more direct approach for eliciting knowledge from these models, since they are language models after all, is prompting, i.e. converting tasks into a language model format. For example, Radford et al. (2019) frame summarization as a language modeling task by appending \"TL;DR:\" to the end of an article and then generating from an LM. Sim- ilarly, Petroni et al. (2019) manually reformulate a knowledge base completion task as a cloze test (i.e., a fill-in-the-blank problem). Compared to existing model analysis methods, prompting is non- invasive: it does not introduce large amounts of additional parameters or require direct inspection of a model\\'s representations. Thus prompting pro- 1\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Original Input xinp a real joy. AUTOPROMPT Xprompt a real joy. atmosphere alot dialogue Clone totally [MASK]. Trigger Tokens Œtrig atmosphere, alot, dialogue, Clone ... Masked LM p([MASK] | @prompt) p(y|xprompt ) Cris marvelous philanthrop + positive Template (xinp, \"trig) {sentence }[T][T][T][T][T][P]. worse incompetence Worse + negative Figure 1: Illustration of AUTOPROMPT applied to probe a masked language model\\'s (MLM\\'s) ability to per- form sentiment analysis. Each input, «inp, is placed into a natural language prompt, «prompt, which contains a single [MASK] token. The prompt is created using a template, A, which combines the original input with a set of trigger tokens, «trig. The trigger tokens are shared across all inputs and determined using a gradient-based search (Section 2.2). Probabilities for each class label, y, are then obtained by marginalizing the MLM predictions, p([MASK]|@prompt), over sets of automatically detected label tokens (Section 2.3). vides a lower bound on what the model \"knows\", and is therefore a more useful analysis tool. How- ever, prompting unfortunately requires manually crafting the context to feed into the model. Not only is this time consuming and non-intuitive for many tasks (e.g., textual entailment), more impor- tantly, models are highly sensitive to this context: improperly-constructed contexts cause artificially low performance (Jiang et al., 2020). Overcoming the need to manually specify prompts would make prompting a more widely useful analysis tool. In this paper, we introduce AUTOPROMPT-an automated method for generating prompts for any task, illustrated in Figure 1. Given a task, e.g., sen- timent analysis, AUTOPROMPT creates a prompt by combining the original task inputs (e.g. reviews) with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al. (2019). The LM predictions for the prompt are converted to class probabilities by marginal- izing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier. We validate the effectiveness of AUTOPROMPT in numerous experiments. First, we use AUTO- PROMPT to construct prompts that test pretrained masked language models (MLMs) on sentiment analysis and natural language inference (NLI). Our tests reveal that, without any finetuning, MLMs perform well on both of these tasks-a properly- prompted RoBERTa achieves 91% accuracy on SST-2 (better than a finetuned ELMo model (Pe- ters et al., 2018)), and 69% accuracy on a bal- anced variant of the SICK-E dataset (Marelli et al., 2014). Next, we apply AUTOPROMPT to the fact re- trieval tasks of LAMA (Petroni et al., 2019), where we are able to construct prompts that more effec- tively elicit MLM\\'s factual knowledge than exist- ing prompts generated using manual and corpus- mining methods. Concretely, we achieve 43.3% precision-at-1, compared to the current best single- prompt result of 34.1% (Jiang et al., 2020). We also introduce a variant of this task, similar to rela- tion extraction (RE), that tests whether MLMs can extract knowledge from a given piece of text. We show that MLMs can actually outperform existing RE models when context sentences with real facts are provided, however, they struggle when context sentences are artificially falsified. Finally, although the goal of AUTOPROMPT is to analyze models, we find that it provides certain practical advantages over finetuning. First, AU- TOPROMPT achieves higher average- and worst- case accuracy than finetuning in low-data regimes. Moreover, unlike finetuning, prompting LMs does not require large amounts of disk space to store model checkpoints; once a prompt is found, it can be used on off-the-shelf pretrained LMs. This is beneficial when serving models for multiple tasks. 2 Overview of AUTOPROMPT A natural way to elicit knowledge from pretrained LMs is to pose tasks as fill-in-the-blank problems.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"However, writing prompts is not only time consum- ing, but it is not clear that the same phrasing will be effective for every model, nor is it clear what crite- ria determine whether a particular phrasing the best to elicit the desired information. In light of this, we introduce AUTOPROMPT, a method that constructs customized prompts for a specific task and MLM of interest, to cause the MLMs to produce the desired knowledge.1 An illustration of AUTOPROMPT is provided in Figure 1. The prompt is constructed by taking the original task inputs-a collection of one or more sequences of tokens (e.g., the review in Figure 1)-and mapping them to a sequence of tokens using a template. In the following sections, we describe how AUTOPROMPT uses labeled train- ing data to construct prompts, and how it uses the output of the MLM as a prediction for the task. 2.1 Background and Notation For the purpose of prompt construction, we distin- guish the original task inputs xinp (e.g., the review in Figure 1, \"a real joy.\") from the prompt prompt (e.g., \"a real joy. atmosphere alot dialogue Clone totally [MASK].\") that is fed into the MLM. The mapping from xinp to @prompt is performed using a template, A. This template defines where each input sequence will be placed in the prompt, as well as the placement of any additional tokens. In particular, it must also define the placement of a special [MASK] token for the MLM to fill in (de- noted by [P] in the template to distinguish it from other [MASK] tokens that might appear). Feeding the prompt into the MLM produces a probability distribution p([MASK]|@prompt) describing which tokens most likely fill in the blank. If class labels naturally correspond to tokens in the vocabulary (e.g., entity names in knowledge base completion tasks), this distribution may be readily interpreted as a distribution over class la- bels. However, for tasks such as sentiment analysis, there may be a set of label tokens Vy that corre- spond to a particular label y. For example, in Fig- ure 1, \"Cris\", \"marvelous\", and \"philanthrop\" all indicate positive sentiment. In this case, the class probability is obtained by marginalizing over the 1 Although we focus only on MLMs in this work, our method is trivially extendable to autoregressive LMs. The only adjustment is that the predict token must occur at the end of the prompt. set of label tokens: p(y|@prompt) = > p([MASK] = w| @prompt) WEVy (1) 2.2 Gradient-Based Prompt Search So far, we have shown how to reformulate a clas- sification task as a language modeling task using prompts. Here, we propose a method for automatic prompt construction based on Wallace et al. (2019). The idea is to add a number of \"trigger\" tokens that are shared across all prompts (denoted by [T] in the example template in Figure 1). These tokens are initialized to [MASK] tokens, and then iteratively updated to maximize the label likelihood (Equa- tion (1)) over batches of examples. Formally, at each step, we compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the jth trigger token ring with another token w E V. Then we identify a candidate set Vcand of the top-k tokens estimated to cause the greatest increase: Vcand = top-k [win V log p(y|xprompt)] WEV (2) where Win is the input embedding of w, and the gradient is taken with respect to the input embed- ding of xtrig. Note that computing this candidate set is roughly as expensive as a single forward pass and backward pass of the model (the dot-products require the same amount of multiplications as com- puting the LM output projection). For each candi- date in this set, we then re-evaluate Equation (1) on the updated prompt, and retain the prompt with the highest probability in the next step-this requires k forward passes of the model. An example prompt produced by this method for the task of sentiment analysis is shown in Figure 1. 2.3 Automating Label Token Selection While in some settings the choice of label tokens is obvious (e.g., when class labels directly correspond to words in the vocabulary), it is less clear what label tokens are appropriate for problems involv- ing more abstract class labels (e.g., NLI). In this section, we develop a general two-step approach to automate the selection of the sets of label tokens Vy. In the first step, we train a logistic classifier to predict the class label using the contextualized embedding of the [MASK] token as input: h = Transformerenc () (3)\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"We write the output of this classifier as: p(y|h(i)) x exp(h(i) . y + By) (4) where y and By are the learned weight and bias terms for the label y, and i represents the index of the [MASK] token. In the second step, we substitute h(i) with the MLM\\'s output word embeddings Wout to obtain a score s(y, w) = p(y|Wout). Intuitively, be- cause Wout . h and y . h are large for words and labels that are relevant to a particular context, Sw X exp (Wout . y + By) should be large for words that are typically associated with a given label. The sets of label tokens are then constructed from the k-highest scoring words: Vy = top-k [s(y, w)] (5) WEV 2.4 Relation to Other Prompting Methods Our work fits into a body of work that probes lan- guage model\\'s knowledge via prompts. Previous works have used manually defined prompts to study an LM\\'s ability to perform: commonsense rea- soning (Trinh and Le, 2018; Kwon et al., 2019; Shwartz et al., 2020), question answering (Lewis et al., 2019), fact recall (Petroni et al., 2019; Jiang et al., 2020; Bouraoui et al., 2019), summariza- tion (Radford et al., 2019), and other supervised tasks (Brown et al., 2020). Schick and Schütze (2020) use manually constructed prompts in con- junction with semi-supervised learning for few- shot learning. We instead automatically create prompts for any task, which leads to higher ac- curacy and opens up new phenomena to analyze. 2.5 Evaluation Setup In the following sections, we apply AUTOPROMPT to probe BERTBASE2 (110M parameters) and ROBERTaLARGE\\'s (355M parameters) knowledge of the following tasks: sentiment analysis, natu- ral language inference (NLI), fact retrieval, and relation extraction. We use the PyTorch imple- mentations and pretrained weights provided by the transformers Python library (Wolf et al., 2019). For sentiment analysis and NLI, we find label to- kens using the logistic-regression-based heuristic described in Section 2.3. For fact retrieval and re- lation extraction, we skip this step as the labels (entities) directly correspond to tokens in the vo- cabulary. For all tasks, we perform the prompt 2For brevity, we will omit subscripts in the model names. search described in Section 2.2 for multiple itera- tions. In each iteration, we use a batch of training data to identify the candidate set Vcand of replace- ment trigger tokens. We then evaluate the label likelihoods of the updated prompts on a separate batch of data, and we retain the best trigger token in the next iteration of the search. At the end of every iteration, we measure the label likelihood on withheld development data, and return the best prompt found during the entire search as the final output. Performance is evaluated using the appro- priate task-specific metrics-e.g., accuracy for sen- timent analysis and NLI, and precision@k for fact retrieval-on a separate withheld test set. Our AUTOPROMPT implementation is publicly available at http://ucinlp.github.io/autoprompt, and supports prompt generation for pretrained mod- els in the HuggingFace transformers library (Wolf et al., 2019) on arbitrary datasets. 3 Sentiment Analysis Sentiment analysis is a fundamental task in NLP, both for natural language understanding research and real-world applications. It is also difficult to probe the extent to which MLMs understand senti- ment without finetuning. Setup We apply our method to convert instances from the binary Stanford Sentiment Treebank (Socher et al., 2013, SST-2) into prompts, using the standard train/test splits. We find label to- kens using a prompt based on the template in Ta- ble 3. For our gradient-based prompt search, we perform a grid search over the following hyperpa- rameters: [Vcand| € {10, 100}, Vy| € {1, 3, 5}, @trig | € [3, 6].3 All prompts are initialized with the same template used to find the label set. We also construct a prompt manually (before automated prompts are generated, to avoid bias) based on the intuition that SST-2 is comprised of movie reviews. We use \"{sentence} this movie was [P].\" as the template, and use \"terrible\" and \"fantastic\" for the negative and positive label to- kens, respectively. Results We show results in Table 1, along with reference scores from the GLUE (Wang et al., 2019) SST-2 leaderboard, and scores for a lin- ear probe trained over the elementwise average of the LM token representations. Prompts gen- erated by AUTOPROMPT reveal that both BERT 3 Required 2 days to run with 8 NVIDIA 2080Ti GPUs.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Finetuned Model BİLSTM - 82.8+ BİLSTM + ELMo 85.2 - 89.3+ BERT (linear probing) 83.4 BERT (finetuned) † RoBERTa (linear probing) 87.9 88.8 RoBERTa (finetuned) - 96.71 BERT (manual) 63.2 BERT (AUTOPROMPT) RoBERTa (manual) 85.3 85.2 RoBERTa (AUTOPROMPT) 91.2 91.4 Dev Test 63.2 - 93.5 80.9 82.3 Table 1: Sentiment Analysis performance on the SST- 2 test set of supervised classifiers (top) and fill-in-the- blank MLMs (bottom). Scores marked with + are from the GLUE leaderboard: http://gluebenchmark.com/ leaderboard. and RoBERTa have a strong knowledge of senti- ment analysis: without any finetuning, BERT per- forms comparably to a supervised BiLSTM, and RoBERTa achieves an accuracy on-par with fine- tuned BERT and ELMo models. In addition, we observe that our automatically constructed prompts are more effective than manual prompts, and that they are difficult to construct using human intuition: the best template for ROBERTa is \"{sentence} at- mosphere alot dialogue Clone totally [P].\" We include results on the effect of the AUTOPROMPT hyperparameters in Appendix A. Accuracy in Low-Data Settings Although the goal of AUTOPROMPT is to probe a model\\'s knowl- edge, we also find that it may be a viable alternative to finetuning in the low-data regime. To show this, we measure the development set accuracy of AU- TOPROMPT prompts when using random subsets of 10, 100, and 1000 instances from the training data. We run our prompt search with |@trig| = 10, Vy| = 3, and | Vcand | = 10. We compare to the performance of BERT and RoBERTa finetuned on the same data. For fair comparison between AU- TOPROMPT and finetuning, we use Mosbach et al. (2020)\\'s recommended parameters for finetuning on small datasets: trained for 20 epochs, using AdamW (Loshchilov and Hutter, 2018) with bias correction and a learning rate that linearly increases to 2 × 10-5 in the first 10% of iterations, and lin- early decreases to 0 afterwards. Experiments are repeated 10 times on random subsets of data (and seeds for the finetuned models). Best-case, worst- case, and average performance are shown in Fig- ure 2. Note that results in the EMNLP version had a bug that has since been fixed. 0.9 Accuracy 0.7 - AutoPrompt 0.5 101 TT 102 103 (a) BERT on SST-2 0.9 Accuracy 0.7 0.5 101 102 103 (b) RoBERTa on SST-2 0.7 - Accuracy 0.5 - 0.3 101 102 103 (c) BERT on SICK-E 0.7 Accuracy 0.5 0.3 101 102 103 (d) RoBERTa on SICK-E Figure 2: Effect of Training Data on sentiment analy- sis and NLI for AUTOPROMPT vs. finetuning. X-axis is the number of data points used during training. Error bars plot the max. and min. accuracies observed over 10 independent runs. (revised since EMNLP version). We observe that while finetuning outperforms AUTOPROMPT on sentiment analysis, AUTO- PROMPT can perform better than finetuning on NLI. Notably, AUTOPROMPT elicits better average per- formance from both BERT and RoBERTa given only 10 training examples. Furthermore, results for RoBERTa are more stable across all sample sizes whereas finetuning can result in \"failed runs\" (con- sistent with Dodge et al. 2020). This behavior in the low-data regime is an interesting phenomenon, and suggests that there are barriers that MLMs must surmount when they are converted to fine- tuned classifiers that are not encountered when the task is presented as masked language modeling. 4 Natural Language Inference To evaluate the semantic understanding of MLMs, we experiment on Natural Language Inference\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Model Majority 56.7 33.3 BERT (linear probing) 68.0 49.5 91.9 RoBERTa (linear probing) 72.6 49.4 91.1 BERT (AUTOPROMPT) 62.3 65.0 69.3 87.3 SICK-E Datasets standard 3-way 2-way 50.0 BERT (finetuned) 86.7 84.0 95.6 55.4 85.7 RoBERTa (AUTOPROMPT) Table 2: Natural Language Inference performance on the SICK-E test set and variants. (Top) Baseline classi- fiers. (Bottom) Fill-in-the-blank MLMs. (NLI). NLI is crucial in many tasks such as reading comprehension and commonsense reasoning (Bow- man et al., 2015), and it is used as a common bench- mark for language understanding. Setup We use the entailment task from the SICK dataset (Marelli et al., 2014, SICK-E) which con- sists of around 10,000 pairs of human-annotated sentences labeled as entailment, contradiction, and neutral. The standard dataset is biased toward the neutral class which represent 56.7% of instances. We also experiment on an unbiased variant with 2-way classification of contradiction vs. entail- ment (2-way), as well as an unbiased 3-way clas- sification variant (3-way). The template used for AUTOPROMPT is provided in Table 3. We search over the following parameters: |Vcand| € {10, 50}, |Vy| E {1, 3, 5, 10}, | trig | € [1, 5], and choose the best prompt according to development set accuracy. Results Table 2 shows that AUTOPROMPT con- siderably outperforms the majority baseline in all experiments. For example, on the 2-way SICK-E dataset, AUTOPROMPT is comparable to a super- vised finetuned BERT. We also test linear probes- linear classifiers trained on top of frozen MLM representations with average pooling -and find AUTOPROMPT has comparable or higher accuracy, despite linear probes being susceptible to false pos- itives. Overall, these results demonstrate that both BERT and RoBERTa have some inherent knowl- edge of natural language inference. - We also examine the efficacy of AUTOPROMPT in the low-data regime (using the same procedure as SST-2) on the unbiased 3-way SICK-E data. The results in Figure 2 show that AUTOPROMPT per- forms on par with finetuned BERT and significantly better than finetuned RoBERTa in low data settings. MLMs Excel on Contradiction We find that the label tokens are more interpretable for con- tradiction compared to entailment or neutral (ex- amples in Table 3). We investigate if this hurts the model performance on entailment and neutral classes. We measure the precision for each la- bel in the 3-way balanced SICK-E dataset. BERT achieves 74.9%, 54.4%, and 36.8% precision for contradiction, entailment, and neutral cases, respec- tively, while RoBERTa obtains 84.9%, 65.1%, and 57.3%. These results suggest that AUTOPROMPT may be more accurate for concepts that can be eas- ily expressed using natural label tokens. 5 Fact Retrieval An important question is whether pretrained MLMs know facts about real-world entities. The LAMA dataset (Petroni et al., 2019) evaluates this using cloze tests that consist of (sub, rel, obj) triples, e.g. (Obama, bornIn, Hawaii), and manually created prompts with missing objects, e.g. \"Obama was born in [MASK].\". LPAQA (Jiang et al., 2020) ex- tends this idea by systematically creating prompts that are generated by mining Wikipedia, paraphras- ing, and crowdsourcing. In this section, we use the same cloze-style setup but automatically gener- ate prompts in order to better evaluate the factual knowledge of MLMs. We compare our approach against LAMA and LPAQA, which are explicitly designed for the task of fact retrieval. Setup We reformulate fact retrieval by mapping (sub,rel,obj) triples to a prompt using the template \"{sub} [T] ... [T][P].\", where the trigger tokens are specific to the relation rel and the correct object obj is the label token. We use the original test set from LAMA (Petroni et al., 2019), henceforth Orig- inal. To collect training data for AUTOPROMPT, we gather at most 1000 facts for each of the 41 re- lations in LAMA from the T-REX dataset (ElSahar et al., 2018). For the relations that still have less than 1000 samples, we gather extra facts straight from Wikidata. We ensure that none of the T-REX triples are present in the test set, and we split the data 80-20 into train and development sets. More- over, because the collected T-REX data is from a slightly different distribution than the LAMA test set, we also consider a separate evaluation where we split the T-REX triples into a 60-20-20 train/dev/test split and evaluate on the test set. This T-REx dataset is used to measure the performance of our prompts when the train and test data is from the same distribution.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Task Prompt Template Sentiment {sentence} [T] ... [T] [P]. Analysis NLI {prem } [P][T] .. . [T]{hyp} Fact Retrieval X plays Y music {sub} [T]. . . [T][P]. Relation X is a Y by profession Extraction { sent } { sub } [T]. . . [T][P]. Prompt found by AUTOPROMPT unflinchingly bleak and desperate Writing academicswhere overseas will appear [MASK]. Two dogs are wrestling and hugging [MASK] concretepathic workplace There is no dog wrestling and hugging Hall Overton fireplacemade antique son alto [MASK]. Leonard Wood (born February 4, 1942) is a former Canadian politician. Leonard Wood gymnasium brotherdicative himself another [MASK]. Label Tokens pos: partnership, extraordinary, ##bla neg: worse, persisted, unconstitutional con: Nobody, nobody, nor ent: ##found, ##ways, Agency neu: ##ponents, ##lary, ##uated Table 3: Example Prompts by AUTOPROMPT for each task. On the left, we show the prompt template, which combines the input, a number of trigger tokens [T], and a prediction token [P]. For classification tasks (sentiment analysis and NLI), we make predictions by summing the model\\'s probability for a number of automatically selected label tokens. For fact retrieval and relation extraction, we take the most likely token predicted by the model. We use AUTOPROMPT with 5 or 7 tokens, and select the search parameters using the T-REX de- velopment set. We prevent proper nouns and to- kens that appear as gold objects in the training data from being selected as trigger tokens. This is done to prevent AUTOPROMPT from \"cheating\" by em- bedding common answers inside the prompt. To evaluate, we observe the rank of the true object in label token distribution of the MLM, and use stan- dard ranking metrics: mean reciprocal rank (MRR), precision-at-1 (P@1), and precision-at-10 (P@10). Results Table 4 shows the performance of MLMs with different prompting methods, and we show qualitative examples in Table 3 and in Appendix C. Prompts generated using AUTOPROMPT can ex- tract factual knowledge from BERT more effec- tively than their manual and mined counterparts: we improve P@1 by up to 12 points. Moreover, despite AUTOPROMPT using only one prompt per relation, it still outperforms LPAQA\\'s ensemble method (which averages predictions for up to 30 prompts) by approximately 4 points. Using 7 trig- ger tokens achieves slightly higher scores than 5 trigger tokens, although the difference is not sub- stantial. This indicates that our approach is stable to the choice of trigger length, which is consis- tent with our sentiment analysis results. Overall, these results show that AUTOPROMPT can retrieve facts more effectively than past prompting meth- ods, thus demonstrating that BERT contains more factual knowledge than previously estimated. Relation Breakdown We also provide a detailed breakdown of the prompts found by Petroni et al. (2019) and AUTOPROMPT, and their associated ac- curacies in Appendix C, Table 7. Manual prompts are competitive when the prompt is easy to specify, e.g., the prompt \"was born in\" for the PLACE OF BIRTH relation. On the other hand, AUTOPROMPT performs especially well for relations that are dif- ficult to specify in a natural language prompt. For example, Petroni et al. (2019)\\'s prompt for the PO- SITION PLAYED ON TEAM relation is \"{sub} plays in [MASK] position\", which is not as specific as the relation requires. Although the prompt from AU- TOPROMPT is not grammatical (\"{sub} ediatric striker ice baseman defensive { obj} \"), it does con- tain tokens that are directly related to sports. BERT outperforms RoBERTa We finally di- rectly compare BERT and RoBERTa. To do so, we subsample the LAMA test set to consist of ex- amples where the object is a single token for both BERT and RoBERTa (Original-RoBERTa).4 BERT actually slightly outperforms RoBERTa, and we find that the prompts generated for RoBERTa tend to contain more irrelevant words (see Appendix C, Table 7). For example, the prompt generated by RoBERTa for the PLAYS INSTRUMENT relation contains words such as \"Trump\" and symbols such as \",\" (),\" for the POSITION PLAYED ON TEAM relation. It is surprising that RoBERTa does not 4The original dataset consists of examples where the object is a single token for BERT.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Prompt Type LAMA LPAQA (Top1) AUTOPROMPT 5 Tokens AUTOPROMPT 7 Tokens 53.89 MRR 40.27 43.57 53.06 Original P@10 P@1 59.49 31.10 35.79 54.29 26.38 62.03 34.10 72.17 42.94 73.93 43.34 54.89 72.02 45.57 MRR P@10 P@1 39.86 54.42 70.80 45.40 T-REX 57.27 31.16 Model BERT 55.22 RoBERTa 49.90 MRR P@10 74.01 68.34 40.01 P@1 45.23 Table 4: Factual Retrieval: On the left, we evaluate BERT on fact retrieval using the Original LAMA dataset from Petroni et al. (2019). For all three metrics (mean reciprocal rank, mean precision-at-10 (P@10), and mean precision-at-1(P@1)), AUTOPROMPT significantly outperforms past prompting methods. We also report results on a T-REx version of the data (see text for details). On the right, we compare BERT versus RoBERTa on a subset of the LAMA data using AUTOPROMPT with 5 tokens. perform better than BERT, and it is worthy of inves- tigating this further in future work. Additionally, recall that prompting is a lower bound on a model\\'s knowledge: the lower relative performance does not mean that the model actually knows less. 6 Relation Extraction Apart from evaluating whether MLMs know facts, it is also important to evaluate whether they can extract knowledge from text. In this section, we use the task of relation extraction (RE)-to identify how entities are related in a given sentence-an important task in information extraction. We create RE prompts in a similar fashion as fact retrieval: for a given triple (subj,rel,obj) and sentence that expresses this relation, we construct a prompt as \"{sent} {sub } [T]. .. [T][P].\", where the trigger to- kens are specific to the relation, and label token is the correct object obj (see Table 3 for an example). Setup We use the T-Rex dataset for RE because each T-REX fact comes with context sentences that mention the subject and object surface forms. We compare AUTOPROMPT to LAMA and LPAQA (their prompts are still useful here), as well as a re- cent supervised relation extraction model (Sorokin and Gurevych, 2017) that was also used by Petroni et al. (2019). To make the evaluation fair for the supervised RE model, we modify the standard RE evaluation. We give the model credit as long as it does not predict a different relation for the subject and object, i.e. we ignore the \"no relation\" pre- diction and all other relations. We also drop all sentences from evaluation for which the model\\'s named entity extractor failed to identify the sub- ject and the object as entities. See Appendix B for further details. For the evaluation of all systems, we treat a prediction as correct if it is either the canonical version of the object (e.g., \"USA\") or the rendered surface form (e.g., \"American\") for any of the context sentences in a given triple. Results T Table 5 shows the results for BERT and RoBERTa. MLMs can extract relational informa- tion more effectively than the supervised RE model, providing up to a 33% increase on the task when using AUTOPROMPT. RoBERTa also outperforms the supervised RE model, although it is worse than BERT (likely for similar reasons as we outline in Section 5). For both BERT and RoBERTa, we no- tice that the trigger tokens consist of words related to their corresponding relations (see Appendix D, Table 8 for full list), e.g. ROBERTa selects \"defy trademarks of namesake manufacturer\" for rela- tion MANUFACTURER/PRODUCER OF PRODUCT. Perturbed Sentence Evaluation A possible ex- planation for the strong results of MLMs in the RE setting is that they may already know many of the relations. Thus, they may directly predict the objects instead of extracting them. To separate this effect, we synthetically perturb the relation extrac- tion dataset by replacing each object in the test data with a random other object and making the same change to the prompt. For example, \"Ryo Kase (born November 9, 1974 in Yokohama >Yorkshire) is a Japanese actor\" where Ryo Kase is the subject, Yokohama is the original object, and Yorkshire is the new object. We regenerate the prompts using the perturbed version of the data. The accuracy of the RE model does not change significantly on the perturbed data (Table 5), how- ever, the accuracy of the MLMs decreases signifi- cantly. This indicates that a significant portion of MLM accuracy comes from background informa- tion rather than relation extraction. Nevertheless, our prompts for BERT outperform their LAMA and LPAQA counterparts, which provides further evi- dence that AUTOPROMPT produces better probes.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Model Supervised RE LSTM 57.95 58.81 BERT (LAMA) 69.06 28.02 BERT (LPAQA) 76.55 30.79 BERT (AUTOPROMPT) 90.73 56.43 RoBERTa (AUTOPROMPT) 60.33 28.95 Original Perturbed Table 5: Relation Extraction: We use prompts to test pretrained MLMs on relation extraction. Compared to a state-of-the-art LSTM model from 2017, MLMs have higher mean precision-at-1 (P@1), especially when us- ing prompts from AUTOPROMPT. We also test models on sentences that have been edited to contain incorrect facts. The accuracy of MLMs drops significantly on these sentences, indicating that their high performance stems from their factual knowledge. 7 Discussion Prompting as an Alternative to Finetuning The goal of prompting a language model is to probe the knowledge that the model acquired from pre- training. Nevertheless, prompting has some prac- tical advantages over finetuning for solving real- world tasks. First, as shown in Section 3, prompts generated using AUTOPROMPT can achieve higher accuracy than finetuning in the low-data regime. Moreover, prompting has advantages over finetun- ing when trying to solve many different tasks (e.g., the many users of the OpenAI GPT-3 API (Brown et al., 2020)). In particular, finetuning requires storing large language model checkpoints for each individual task, and drastically increases system cost and complexity because it requires deploying many different models at the same time. Prompt- ing alleviates both of these issues. Only prompts are stored for each individual task, while the same pretrained model is used across all of the tasks. Limitations of Prompting There are certain phenomena that are difficult to elicit from pre- trained language models via prompts. In our pre- liminary evaluation on datasets such as QQP (Iyer et al., 2017) and RTE (Dagan et al., 2005), prompts generated manually and with AUTOPROMPT did not perform considerably better than chance. How- ever, we cannot conclude that BERT does not know paraphrasing or entailment from these results. In general, different probing methods have different tasks and phenomena they are suitable for: AUTO- PROMPT makes prompt-based probes more gener- ally applicable, but, it still remains just one tool in the toolbox of the interpretability researcher. Limitations of AUTOPROMPT One downside of AUTOPROMPT is that it requires labeled train- ing data. Although this is also required for other probing techniques (e.g., linear probing classi- fiers), manual prompts rely on domain/language insights instead of labeled data. Compared to human-designed prompts, AUTOPROMPT gener- ated prompts lack interpretability, which is similar to other probing techniques, such as linear probing classifiers. Another limitation of AUTOPROMPT is that it can sometimes struggle when the training data is highly imbalanced. For example, in Sec- tions 4 and 5 we show that the prompts often just increase the likelihood of the majority label. Re- balancing the training data can help to mitigate this problem. Finally, due to the greedy search over the large discrete space of phrases, AUTOPROMPT is sometimes brittle; we leave more effective crafting techniques for future directions. 8 Conclusion In this paper, we introduce AUTOPROMPT, an approach to develop automatically-constructed prompts that elicit knowledge from pretrained MLMs for a variety of tasks. We show that these prompts outperform manual prompts while requir- ing less human effort. Furthermore, the results for sentiment analysis and textual entailment sug- gest that, in some data-scarce settings, it may be more effective to prompt language models than to finetune them for the task. Although we fo- cus only on masked language models in this paper, our method can be trivially extended to standard language models, and thus maybe useful for con- structing inputs for models like GPT-3 (Brown et al., 2020). Source code and datasets to re- produce the results in this paper is available at http://ucinlp.github.io/autoprompt. Acknowledgments We would like to thank the LAMA and LPAQA teams for answering our questions. We would also like to thank the members of UCI NLP, Matt Gard- ner, Sebastian Riedel, and Antoine Bosselut for valuable feedback. This material is based upon work sponsored by the DARPA MCS program un- der Contract No. N660011924033 with the United States Office Of Naval Research.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"References Zied Bouraoui, Jose Camacho-Collados, and Steven Schockaert. 2019. Inducing relational knowledge from BERT. In AAAI. Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv: 2005.14165. Alexis Conneau, Germán Kruszewski, Guillaume Lam- ple, Loïc Barrault, and Marco Baroni. 2018. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In ACL. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges Work- shop. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language under- standing. In NAACL. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stop- ping. arXiv preprint arXiv:2002.06305. Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl. 2018. T-REX: A large scale alignment of natural language with knowledge base triples. In LREC. John Hewitt and Percy Liang. 2019. Designing and interpreting probes with control tasks. In EMNLP. Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. 2017. First quora dataset release: Question pairs. Sarthak Jain and Byron C Wallace. 2019. Attention is not explanation. In NAACL. Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? In TACL. Sunjae Kwon, Cheongwoong Kang, Jiyeon Han, and Jaesik Choi. 2019. Why do masked neural language models still need common sense knowledge? arXiv preprint arXiv: 1911.03024. Patrick Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019. Unsupervised question answering by cloze translation. In ACL. Nelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters, and Noah A Smith. 2019. Linguis- tic knowledge and transferability of contextual rep- resentations. In NAACL. Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Con- ference on Learning Representations. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, Roberto Zamparelli, et al. 2014. A SICK cure for the evaluation of com- positional distributional semantic models. In LREC. Marius Mosbach, Maksym Andriushchenko, and Diet- rich Klakow. 2020. On the stability of fine-tuning bert: Misconceptions, explanations, and strong base- lines. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In NAACL. Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Se- bastian Riedel. 2019. Language models as knowl- edge bases? In EMNLP. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Techni- cal report. Timo Schick and Hinrich Schütze. 2020. Exploit- ing cloze questions for few-shot text classification and natural language inference. arXiv preprint arXiv:2001.07676. Vered Shwartz, Peter West, Ronan Le Bras, Chan- dra Bhagavatula, and Yejin Choi. 2020. Unsuper- vised commonsense question answering with self- talk. arXiv preprint arXiv: 2004.05483. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In EMNLP. Daniil Sorokin and Iryna Gurevych. 2017. Context- aware representations for knowledge base relation extraction. In EMNLP. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv: 1806.02847. Elena Voita and Ivan Titov. 2020. Information- theoretic probing with minimum description length. In EMNLP. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial trig- gers for attacking and analyzing NLP. In EMNLP.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In ICLR. Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In EMNLP. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Fun- towicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Can- wen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. HuggingFace\\'s Transformers: State-of-the- art natural language processing. arXiv preprint arXiv: 1910.03771.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"A Effect of Hyperparameters on Sentiment Analysis BERT 0.90 0.85 - Accuracy 0.80 0.75 JCtrig| =3 ICtrig | = 4 @trig |=5 CCtrig |=6 RoBERTa 0.90 0.85 - Accuracy 0.80 0.75 1 2 3 4 5 Label Set Size, |Vy| Figure 3: Effect of Label and Trigger Set Sizes on sentiment analysis. The number of candidate replace- ments is fixed at | Vcand| = 100. Increasing the label set size improves performance, while changing the trigger length does not have much impact. To measure the effects of the AUTOPROMPT search hyperparameters, we plot the validation ac- curacy as a function of label set size | Vy| and the number of trigger tokens |@trig| in Figure 3. We fix the number of candidates at | Vcand| = 100. We observe similar trends when | Vcand| = 10. Varying the number of trigger tokens generally has little effect. On the other hand, there is a sub- stantial increase in accuracy when increasing the label set size from 1 to 3 (approximately +5% for BERT, and +10% for RoBERTa). After analyzing the label sets, we find that our method generally produces intuitive results-\"marvelous\" and \"phi- lanthrop\" are associated with positive sentiment, whereas \"worse\" and \"incompetence\" are associ- ated with negative sentiment for RoBERTa. B Relation Extraction Details Following Petroni et al. (2019), we use the pre- trained RE model from Sorokin and Gurevych (2017) as our baseline. To encode the sentence, this model uses a combination of an LSTM-based relation encoder and an attention mechanism. To make predictions, the model constructs a knowl- edge graph whose edges are the extracted relation triples. The standard RE evaluation measures how well the model predicts the relation types of entity pairs on the sentence level. Since our goal is to extract the object of relation triplets, rather than the relation itself, we tweak the standard RE evaluation. We feed the RE model sentences from test facts and we query the resulting graph for all edges that contain the given subject and relation. Then we select the triple with the highest confidence and compare it\\'s object to the gold object. We do this for every fact and take the average across all relations to get the overall preci- sion. The RE model is not trained to predict two of the original T-REX relations. For fair comparison, we exclude these two relations for our evaluation.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"C Additional Fact Retrieval Results Relation Manual Prompt (LAMA) P1001 P101 [X] works in the field of [Y] 864 9.91 P103 The native language of [X] is [Y] P106 [X] is a [Y] by profession P108 [X] works for [Y] 5.74 P127 [X] is owned by [Y] 32.46 P1303 P131 [X] is located in [Y] 1000 23.27 P136 [X] plays [Y] music 1000 0.75 P1376 [X] is the capital of [Y] 310 73.93 P138 [X] is named after [Y] 856 61.55 59.69 P140 [X] is affiliated with the [Y] religion 445 0.63 59.83 P1412 [X] used to communicate in [Y] 1000 65.02 P159 P17 P176 P178 [X] is developed by [Y] 560 62.84 59.12 P19 [X] was born in [Y] 1000 21.08 20.87 P190 [X] and [Y] are twin cities 895 P20 P264 [X] is represented by music label [Y] 1000 9.56 10.26 43.82 P27 [X] is [Y] citizen 1000 0.0 P276 [X] is located in [Y] 1000 41.5 41.5 44.11 P279 [X] is a subclass of [Y] 1000 30.74 14.75 54.93 P30 [X] is located in [Y] P31 [X] is a [Y] 36.66 P36 The capital of [X] is [Y] 1000 62.16 62.16 60.6 P361 [X] is part of [Y] P364 The original language of [X] is [Y] 1000 P37 The official language of [X] is [Y] 311 56.83 62.63 P39 [X] has the position of [Y] 7.96 P407 [X] was written in [Y] 68.42 P413 [X] plays in [Y] position P449 [X] was originally aired on [Y] P463 [X] is a member of [Y] 679 P47 [X] shares border with [Y] 1000 P495 P527 [X] consists of [Y] P530 [X] maintains diplomatic relations with [Y] 927 P740 [X] was founded in [Y] 1000 P937 [X] used to work in [Y] 39.1 [X] is a legal term in [Y] 1000 [X] plays [Y] 1000 7.59 The headquarter of [X] is in [Y] 1000 32.37 35.57 35.47 [X] is located in [Y] 1000 31.29 [X] is produced by [Y] 1000 85.64 81.67 [X] died in [Y] [X] was created in [Y] 1000 #train 1000 72.16 1000 0.63 376 6.79 548 34.79 1000 27.91 27.91 31.16 1000 25.44 18.56 70.36 1000 36.66 51.95 1000 23.61 31.44 17.7 1000 16.14 30.72 1000 59.18 65.22 1000 0.53 1000 20.89 9.08 34.39 1000 1000 29.77 LAMA LPAQA 70.47 2.41 44.51 43.93 48.48 54.55 67.11 57.33 54.22 13.67 13.34 19.52 16.5 32.23 36.63 11.07 10.55 25.61 2.81 3.92 3.11 7.59 AUTOPROMPT 72.75 5.32 12.79 72.16 0.0 18.02 22.81 16.76 59.83 64.71 35.48 52.15 1.91 41.51 46.69 23.74 41.7 13.68 13.89 82.45 82.09 14.72 8.62 35.95 15.38 37.46 55.42 40.17 66.05 75.26 71.21 87.78 66.72 19.92 2.31 38.36 Table 6: A breakdown of all relations for fact retrieval on the original dataset from Petroni et al. (2019). We compare P@1 of prompts generated by LAMA, LPAQA, and our approach using five prompt tokens.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Relation Method P101 Manual AUTOPROMPT BERT AUTOPROMPT RoBERTa P103 Manual AUTOPROMPT BERT P106 Manual [X] is a [Y] by profession 0.73 P127 Manual [X] is owned by [Y] 36.67 AUTOPROMPT BERT X] is hindwings mainline architecture within [Y] 47.01 AUTOPROMPT RoBERTa [X] picThom unwillingness officially governs [Y] 39.58 P1303 Manual [X] plays [Y] 18.91 AUTOPROMPT BERT AUTOPROMPT RoBERTa [X] playingdrum concertoative electric [Y] 42.69 [X]Trump learned soloKeefe classical [Y] 44.44 P136 Manual [X] plays [Y] music 0.7 AUTOPROMPT BERT [X] freaking genre orchestra fiction acid [Y] 59.95 AUTOPROMPT RoBERTa [X] blends postwar hostage drama sax [Y] 52.97 P1376 Manual [X] is the capital of [Y] 81.11 AUTOPROMPT BERT AUTOPROMPT RoBERTa P178 Manual [X] is developed by [Y] 62.76 AUTOPROMPT BERT AUTOPROMPT RoBERTa X] is memory arcade branding by [Y] 64.45 [X] 1987 floppy simulator users sued [Y] 69.56 P20 Manual [X] died in [Y] 32.07 AUTOPROMPT BERT X] reorganizationotype photographic studio in [Y] 33.53 AUTOPROMPT RoBERTa [X] .. enigmatic twentieth nowadays near [Y] 31.33 P27 Manual [X] is [Y] citizen 0.0 AUTOPROMPT BERT AUTOPROMPT RoBERTa P276 Manual [X] is located in [Y] 43.73 AUTOPROMPT BERT AUTOPROMPT RoBERTa P279 Manual [X] is a subclass of [Y] 31.04 AUTOPROMPT BERT AUTOPROMPT RoBERTa [X] is î adequately termed coated [Y] 55.65 [X], formerly prayers unstaceous [Y] 52.55 P37 Manual AUTOPROMPT BERT AUTOPROMPT RoBERTa P407 Manual AUTOPROMPT BERT AUTOPROMPT RoBERTa [X] was written in [Y] 60.21 [X] playedić every dialect but [Y] 69.31 [X] scaven pronunciation. * Wikipedia speaks [Y] 72.0 P413 AUTOPROMPT RoBERTa AUTOPROMPT BERT [X] supporters studied politicians musician turned [Y] 15.83 AUTOPROMPT RoBERTa [X] (), astronomers businessman.former [Y] 19.24 Manual AUTOPROMPT RoBERTa Prompt P@1 [X] works in the field of [Y] 11.52 [X] probability earliest fame totaled studying [Y] 15.01 [X] 1830 dissertation applying mathsucci [Y] 0.17 The native language of [X] is [Y] 74.54 [X]PA communerug speaks proper [Y] 84.87 [X]neau optionally fluent! ? \\'traditional [Y] 81.61 X] boasts native territory traditionally called [Y] 63.33 [X] limestone depositedati boroughDepending [Y] 28.33 [X] m3 badminton pieces internationally representing [Y] 46.13 [X] offic organise forests statutes northwestern [Y] 42.07 [X] consists kilograms centred neighborhoods in [Y] 44.64 [X] manoeuv constructs whistleblowers hills near [Y] 37.47 The official language of [X] is [Y] 56.89 [X]inen dialects resembled officially exclusively [Y] 54.44 [X]onen tribes descending speak mainly [Y] 53.67 [X] plays in [Y] position 0.53 AUTOPROMPT BERT [X] played colors skier <> defensive [Y] 41.71 [X],\" (), ex-, Liverpool [Y] 23.21 Table 7: Examples of manual prompts (first line, shown with BERT\\'s P@1) and prompts generated via AUTO- PROMPT for Fact Retrieval.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"D Additional Relation Extraction Results Relation P103 (native language) P36 (capital) P530 (diplomatic relation) BERT P106 (occupation) RoBERTa Spencer Treat Clark (born September 24, 1987) is an Amer- Hulk ican actor who has appeared in several films, including Glad- iator, Mystic River, and Unbreakable. Spencer Treat Clark famously the famously handsome the [MASK]. P276 (location) P176 (manufacturer) RoBERTa P279 (subclass of) BERT Mizeria is a Polish saladsandwich consisting of thinly sliced or grated cucumbers, often with sour cream though in some cases oil. Mizeria is calls direcend altitude [MASK]. P463 (member of) Model BERT RoBERTa Kirk was born in Clinton County, Ohio, and he entered ser- Wilmington vice in Wilmington, Ohio. Clinton County famously includes the zoo influencing [MASK]. BERT RoBERTa Context and Prompt Prediction Alexandra Lamy (born 14 October 1971) is a French actress. Alexandra Lamy speaks airfield dripping % of [MASK]. French The Black Sea forms in an east-west trending elliptical de- pression which lies between Bulgaria, Georgia, Romania, Russia, Turkey, and Ukraine. Ukraine qualified some im- migration actually entered [MASK]. Russia The Immortal Game was a chess game played by Adolf Anderssen and Lionel Kieseritzky on 21 June 1851 in Len- denSeoul, during a break of the first international tourna- ment. The Immortal Game locatedstered regardless streets in [MASK]. The Honda Civic del Sol is a 2-seater front-engined, front wheel drive, targa top car manufactured by Honda Toyota in the 1990s. Honda Civic del Sol defy trademarks of name- sake manufacturer [MASK]. Seoul Toyota food RushAerosmith was a Canadian rock band consisting of Kiss Geddy Lee (bass, vocals, keyboards), Alex Lifeson (guitars), and Neil Peart (drums, percussion, lyricist). Alex Lifeson affiliatedalach the internationally initials [MASK]. Table 8: Examples of prompts generated using AUTOPROMPT for relation extraction. Underlined words represent the gold object. The bottom half of the Table shows examples of our augmented evaluation where the original objects (represented by crossed-out words) are replaced by new objects.\"\\n\\n\\nCONCISE SUMMARY:']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc_sum \u001b[38;5;241m=\u001b[39m \u001b[43msummary_eng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_summary_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEXTRACTED_DATA_FOLDER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSUMMARY_FOLDER\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\code\\azure-openai-samples\\samples\\langchain\\legacy\\AzureOpenAIUtil\\Summerization.py:29\u001b[0m, in \u001b[0;36msummarization.generate_summary_files\u001b[1;34m(self, source_folder, destination_folder, summerization_type, verbose)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerate summary for file:\u001b[39m\u001b[38;5;124m'\u001b[39m, file)\n\u001b[1;32m---> 29\u001b[0m file_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummerization_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummerization_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m summary \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m: file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummerization_type\u001b[39m\u001b[38;5;124m'\u001b[39m:summerization_type, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m:file_summary}\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(destination_folder, file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\code\\azure-openai-samples\\samples\\langchain\\legacy\\AzureOpenAIUtil\\Summerization.py:20\u001b[0m, in \u001b[0;36msummarization.get_summary\u001b[1;34m(self, file, summerization_type)\u001b[0m\n\u001b[0;32m     18\u001b[0m docs \u001b[38;5;241m=\u001b[39m [Document(page_content \u001b[38;5;241m=\u001b[39m page[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m file_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     19\u001b[0m chain \u001b[38;5;241m=\u001b[39m load_summarize_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, chain_type\u001b[38;5;241m=\u001b[39msummerization_type)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\chains\\base.py:505\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    506\u001b[0m         _output_key\n\u001b[0;32m    507\u001b[0m     ]\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    511\u001b[0m         _output_key\n\u001b[0;32m    512\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\chains\\base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    311\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    312\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    314\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\chains\\base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    297\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    298\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    299\u001b[0m     inputs,\n\u001b[0;32m    300\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    307\u001b[0m     )\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:122\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    121\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 122\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\chains\\combine_documents\\map_reduce.py:224\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[1;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    214\u001b[0m     docs: List[Document],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    218\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m     map_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m     question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[0;32m    230\u001b[0m     result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    231\u001b[0m         Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[0;32m    234\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\chains\\llm.py:232\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    231\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n\u001b[0;32m    234\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\chains\\llm.py:229\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    224\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    225\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    226\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_list},\n\u001b[0;32m    227\u001b[0m )\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 229\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    231\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\chains\\llm.py:120\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    118\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    127\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    128\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    129\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\llms\\base.py:507\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    501\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    505\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    506\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\llms\\base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    642\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    643\u001b[0m         )\n\u001b[0;32m    644\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    646\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    654\u001b[0m         )\n\u001b[0;32m    655\u001b[0m     ]\n\u001b[1;32m--> 656\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\llms\\base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    543\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    545\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\llms\\base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    523\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    528\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    530\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 531\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    534\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    535\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    536\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    539\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    540\u001b[0m         )\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\llms\\openai.py:1125\u001b[0m, in \u001b[0;36mOpenAIChat._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]])\n\u001b[1;32m-> 1125\u001b[0m messages, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chat_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1126\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m   1127\u001b[0m full_response \u001b[38;5;241m=\u001b[39m completion_with_retry(\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28mself\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessages, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[0;32m   1129\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\louisli\\AppData\\Local\\miniconda3\\envs\\openai_0\\Lib\\site-packages\\langchain\\llms\\openai.py:1054\u001b[0m, in \u001b[0;36mOpenAIChat._get_chat_params\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_chat_params\u001b[39m(\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompts: List[\u001b[38;5;28mstr\u001b[39m], stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple:\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1054\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1055\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIChat currently only supports single prompt, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1056\u001b[0m         )\n\u001b[0;32m   1057\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_messages \u001b[38;5;241m+\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompts[\u001b[38;5;241m0\u001b[39m]}]\n\u001b[0;32m   1058\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params}\n",
      "\u001b[1;31mValueError\u001b[0m: OpenAIChat currently only supports single prompt, got ['Write a concise summary of the following:\\n\\n\\n\"AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts Yasaman Razeghi *< Taylor Shin* Robert L. Logan IV* Eric Wallace Sameer Singh University of California, Irvine *University of California, Berkeley {tshin1, yrazeghi, rlogan, sameer } @uci.edu ericwallace@berkeley.edu Abstract The remarkable success of pretrained lan- guage models has motivated the study of what kinds of knowledge these models learn dur- ing pretraining. Reformulating tasks as fill- in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suit- able prompts. To address this, we develop AUTOPROMPT, an automated method to cre- ate prompts for a diverse set of tasks, based on a gradient-guided search. Using AUTO- PROMPT, we show that masked language mod- els (MLMs) have an inherent capability to perform sentiment analysis and natural lan- guage inference without additional parame- ters or finetuning, sometimes achieving per- formance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowl- edge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that au- tomatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a re- placement for finetuning. arXiv:2010.15980v2 [cs.CL] 7 Nov 2020 1 Introduction Pretrained language models (LMs) have had ex- ceptional success when adapted to downstream tasks via finetuning (Peters et al., 2018; Devlin et al., 2019). Although it is clear that pretrain- ing improves accuracy, it is difficult to determine whether the knowledge that finetuned LMs contain is learned during the pretraining or the finetuning * First three authors contributed equally. process. How can we directly evaluate the knowl- edge present in pretrained LMs, be it linguistic, factual, commonsense, or task-specific? Numerous techniques have been proposed to elicit such knowledge by analyzing pretrained LMs\\' internal representations. A common strategy is to use probing classifiers-shallow classifiers that predict certain attributes using an LMs\\' representa- tions as features (Conneau et al., 2018; Liu et al., 2019). However, probing classifiers require ad- ditional learned parameters and are thus suscep- tible to false positives; high probing accuracy is not a sufficient condition to conclude that an LM contains a certain piece of knowledge (Hewitt and Liang, 2019; Voita and Titov, 2020). Attention visualization, another common technique, has a similar failure mode: attention scores may be corre- lated with, but not caused by the underlying target knowledge, leading to criticism against their use as explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Both probing and attention visu- alizations also struggle to evaluate knowledge that cannot be represented as simple token- or sequence- level classification tasks. A more direct approach for eliciting knowledge from these models, since they are language models after all, is prompting, i.e. converting tasks into a language model format. For example, Radford et al. (2019) frame summarization as a language modeling task by appending \"TL;DR:\" to the end of an article and then generating from an LM. Sim- ilarly, Petroni et al. (2019) manually reformulate a knowledge base completion task as a cloze test (i.e., a fill-in-the-blank problem). Compared to existing model analysis methods, prompting is non- invasive: it does not introduce large amounts of additional parameters or require direct inspection of a model\\'s representations. Thus prompting pro- 1\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Original Input xinp a real joy. AUTOPROMPT Xprompt a real joy. atmosphere alot dialogue Clone totally [MASK]. Trigger Tokens Œtrig atmosphere, alot, dialogue, Clone ... Masked LM p([MASK] | @prompt) p(y|xprompt ) Cris marvelous philanthrop + positive Template (xinp, \"trig) {sentence }[T][T][T][T][T][P]. worse incompetence Worse + negative Figure 1: Illustration of AUTOPROMPT applied to probe a masked language model\\'s (MLM\\'s) ability to per- form sentiment analysis. Each input, «inp, is placed into a natural language prompt, «prompt, which contains a single [MASK] token. The prompt is created using a template, A, which combines the original input with a set of trigger tokens, «trig. The trigger tokens are shared across all inputs and determined using a gradient-based search (Section 2.2). Probabilities for each class label, y, are then obtained by marginalizing the MLM predictions, p([MASK]|@prompt), over sets of automatically detected label tokens (Section 2.3). vides a lower bound on what the model \"knows\", and is therefore a more useful analysis tool. How- ever, prompting unfortunately requires manually crafting the context to feed into the model. Not only is this time consuming and non-intuitive for many tasks (e.g., textual entailment), more impor- tantly, models are highly sensitive to this context: improperly-constructed contexts cause artificially low performance (Jiang et al., 2020). Overcoming the need to manually specify prompts would make prompting a more widely useful analysis tool. In this paper, we introduce AUTOPROMPT-an automated method for generating prompts for any task, illustrated in Figure 1. Given a task, e.g., sen- timent analysis, AUTOPROMPT creates a prompt by combining the original task inputs (e.g. reviews) with a collection of trigger tokens according to a template. The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al. (2019). The LM predictions for the prompt are converted to class probabilities by marginal- izing over a set of associated label tokens, which can either be learned or specified ahead of time, enabling the LM to be evaluated the same as one would any other classifier. We validate the effectiveness of AUTOPROMPT in numerous experiments. First, we use AUTO- PROMPT to construct prompts that test pretrained masked language models (MLMs) on sentiment analysis and natural language inference (NLI). Our tests reveal that, without any finetuning, MLMs perform well on both of these tasks-a properly- prompted RoBERTa achieves 91% accuracy on SST-2 (better than a finetuned ELMo model (Pe- ters et al., 2018)), and 69% accuracy on a bal- anced variant of the SICK-E dataset (Marelli et al., 2014). Next, we apply AUTOPROMPT to the fact re- trieval tasks of LAMA (Petroni et al., 2019), where we are able to construct prompts that more effec- tively elicit MLM\\'s factual knowledge than exist- ing prompts generated using manual and corpus- mining methods. Concretely, we achieve 43.3% precision-at-1, compared to the current best single- prompt result of 34.1% (Jiang et al., 2020). We also introduce a variant of this task, similar to rela- tion extraction (RE), that tests whether MLMs can extract knowledge from a given piece of text. We show that MLMs can actually outperform existing RE models when context sentences with real facts are provided, however, they struggle when context sentences are artificially falsified. Finally, although the goal of AUTOPROMPT is to analyze models, we find that it provides certain practical advantages over finetuning. First, AU- TOPROMPT achieves higher average- and worst- case accuracy than finetuning in low-data regimes. Moreover, unlike finetuning, prompting LMs does not require large amounts of disk space to store model checkpoints; once a prompt is found, it can be used on off-the-shelf pretrained LMs. This is beneficial when serving models for multiple tasks. 2 Overview of AUTOPROMPT A natural way to elicit knowledge from pretrained LMs is to pose tasks as fill-in-the-blank problems.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"However, writing prompts is not only time consum- ing, but it is not clear that the same phrasing will be effective for every model, nor is it clear what crite- ria determine whether a particular phrasing the best to elicit the desired information. In light of this, we introduce AUTOPROMPT, a method that constructs customized prompts for a specific task and MLM of interest, to cause the MLMs to produce the desired knowledge.1 An illustration of AUTOPROMPT is provided in Figure 1. The prompt is constructed by taking the original task inputs-a collection of one or more sequences of tokens (e.g., the review in Figure 1)-and mapping them to a sequence of tokens using a template. In the following sections, we describe how AUTOPROMPT uses labeled train- ing data to construct prompts, and how it uses the output of the MLM as a prediction for the task. 2.1 Background and Notation For the purpose of prompt construction, we distin- guish the original task inputs xinp (e.g., the review in Figure 1, \"a real joy.\") from the prompt prompt (e.g., \"a real joy. atmosphere alot dialogue Clone totally [MASK].\") that is fed into the MLM. The mapping from xinp to @prompt is performed using a template, A. This template defines where each input sequence will be placed in the prompt, as well as the placement of any additional tokens. In particular, it must also define the placement of a special [MASK] token for the MLM to fill in (de- noted by [P] in the template to distinguish it from other [MASK] tokens that might appear). Feeding the prompt into the MLM produces a probability distribution p([MASK]|@prompt) describing which tokens most likely fill in the blank. If class labels naturally correspond to tokens in the vocabulary (e.g., entity names in knowledge base completion tasks), this distribution may be readily interpreted as a distribution over class la- bels. However, for tasks such as sentiment analysis, there may be a set of label tokens Vy that corre- spond to a particular label y. For example, in Fig- ure 1, \"Cris\", \"marvelous\", and \"philanthrop\" all indicate positive sentiment. In this case, the class probability is obtained by marginalizing over the 1 Although we focus only on MLMs in this work, our method is trivially extendable to autoregressive LMs. The only adjustment is that the predict token must occur at the end of the prompt. set of label tokens: p(y|@prompt) = > p([MASK] = w| @prompt) WEVy (1) 2.2 Gradient-Based Prompt Search So far, we have shown how to reformulate a clas- sification task as a language modeling task using prompts. Here, we propose a method for automatic prompt construction based on Wallace et al. (2019). The idea is to add a number of \"trigger\" tokens that are shared across all prompts (denoted by [T] in the example template in Figure 1). These tokens are initialized to [MASK] tokens, and then iteratively updated to maximize the label likelihood (Equa- tion (1)) over batches of examples. Formally, at each step, we compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the jth trigger token ring with another token w E V. Then we identify a candidate set Vcand of the top-k tokens estimated to cause the greatest increase: Vcand = top-k [win V log p(y|xprompt)] WEV (2) where Win is the input embedding of w, and the gradient is taken with respect to the input embed- ding of xtrig. Note that computing this candidate set is roughly as expensive as a single forward pass and backward pass of the model (the dot-products require the same amount of multiplications as com- puting the LM output projection). For each candi- date in this set, we then re-evaluate Equation (1) on the updated prompt, and retain the prompt with the highest probability in the next step-this requires k forward passes of the model. An example prompt produced by this method for the task of sentiment analysis is shown in Figure 1. 2.3 Automating Label Token Selection While in some settings the choice of label tokens is obvious (e.g., when class labels directly correspond to words in the vocabulary), it is less clear what label tokens are appropriate for problems involv- ing more abstract class labels (e.g., NLI). In this section, we develop a general two-step approach to automate the selection of the sets of label tokens Vy. In the first step, we train a logistic classifier to predict the class label using the contextualized embedding of the [MASK] token as input: h = Transformerenc () (3)\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"We write the output of this classifier as: p(y|h(i)) x exp(h(i) . y + By) (4) where y and By are the learned weight and bias terms for the label y, and i represents the index of the [MASK] token. In the second step, we substitute h(i) with the MLM\\'s output word embeddings Wout to obtain a score s(y, w) = p(y|Wout). Intuitively, be- cause Wout . h and y . h are large for words and labels that are relevant to a particular context, Sw X exp (Wout . y + By) should be large for words that are typically associated with a given label. The sets of label tokens are then constructed from the k-highest scoring words: Vy = top-k [s(y, w)] (5) WEV 2.4 Relation to Other Prompting Methods Our work fits into a body of work that probes lan- guage model\\'s knowledge via prompts. Previous works have used manually defined prompts to study an LM\\'s ability to perform: commonsense rea- soning (Trinh and Le, 2018; Kwon et al., 2019; Shwartz et al., 2020), question answering (Lewis et al., 2019), fact recall (Petroni et al., 2019; Jiang et al., 2020; Bouraoui et al., 2019), summariza- tion (Radford et al., 2019), and other supervised tasks (Brown et al., 2020). Schick and Schütze (2020) use manually constructed prompts in con- junction with semi-supervised learning for few- shot learning. We instead automatically create prompts for any task, which leads to higher ac- curacy and opens up new phenomena to analyze. 2.5 Evaluation Setup In the following sections, we apply AUTOPROMPT to probe BERTBASE2 (110M parameters) and ROBERTaLARGE\\'s (355M parameters) knowledge of the following tasks: sentiment analysis, natu- ral language inference (NLI), fact retrieval, and relation extraction. We use the PyTorch imple- mentations and pretrained weights provided by the transformers Python library (Wolf et al., 2019). For sentiment analysis and NLI, we find label to- kens using the logistic-regression-based heuristic described in Section 2.3. For fact retrieval and re- lation extraction, we skip this step as the labels (entities) directly correspond to tokens in the vo- cabulary. For all tasks, we perform the prompt 2For brevity, we will omit subscripts in the model names. search described in Section 2.2 for multiple itera- tions. In each iteration, we use a batch of training data to identify the candidate set Vcand of replace- ment trigger tokens. We then evaluate the label likelihoods of the updated prompts on a separate batch of data, and we retain the best trigger token in the next iteration of the search. At the end of every iteration, we measure the label likelihood on withheld development data, and return the best prompt found during the entire search as the final output. Performance is evaluated using the appro- priate task-specific metrics-e.g., accuracy for sen- timent analysis and NLI, and precision@k for fact retrieval-on a separate withheld test set. Our AUTOPROMPT implementation is publicly available at http://ucinlp.github.io/autoprompt, and supports prompt generation for pretrained mod- els in the HuggingFace transformers library (Wolf et al., 2019) on arbitrary datasets. 3 Sentiment Analysis Sentiment analysis is a fundamental task in NLP, both for natural language understanding research and real-world applications. It is also difficult to probe the extent to which MLMs understand senti- ment without finetuning. Setup We apply our method to convert instances from the binary Stanford Sentiment Treebank (Socher et al., 2013, SST-2) into prompts, using the standard train/test splits. We find label to- kens using a prompt based on the template in Ta- ble 3. For our gradient-based prompt search, we perform a grid search over the following hyperpa- rameters: [Vcand| € {10, 100}, Vy| € {1, 3, 5}, @trig | € [3, 6].3 All prompts are initialized with the same template used to find the label set. We also construct a prompt manually (before automated prompts are generated, to avoid bias) based on the intuition that SST-2 is comprised of movie reviews. We use \"{sentence} this movie was [P].\" as the template, and use \"terrible\" and \"fantastic\" for the negative and positive label to- kens, respectively. Results We show results in Table 1, along with reference scores from the GLUE (Wang et al., 2019) SST-2 leaderboard, and scores for a lin- ear probe trained over the elementwise average of the LM token representations. Prompts gen- erated by AUTOPROMPT reveal that both BERT 3 Required 2 days to run with 8 NVIDIA 2080Ti GPUs.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Finetuned Model BİLSTM - 82.8+ BİLSTM + ELMo 85.2 - 89.3+ BERT (linear probing) 83.4 BERT (finetuned) † RoBERTa (linear probing) 87.9 88.8 RoBERTa (finetuned) - 96.71 BERT (manual) 63.2 BERT (AUTOPROMPT) RoBERTa (manual) 85.3 85.2 RoBERTa (AUTOPROMPT) 91.2 91.4 Dev Test 63.2 - 93.5 80.9 82.3 Table 1: Sentiment Analysis performance on the SST- 2 test set of supervised classifiers (top) and fill-in-the- blank MLMs (bottom). Scores marked with + are from the GLUE leaderboard: http://gluebenchmark.com/ leaderboard. and RoBERTa have a strong knowledge of senti- ment analysis: without any finetuning, BERT per- forms comparably to a supervised BiLSTM, and RoBERTa achieves an accuracy on-par with fine- tuned BERT and ELMo models. In addition, we observe that our automatically constructed prompts are more effective than manual prompts, and that they are difficult to construct using human intuition: the best template for ROBERTa is \"{sentence} at- mosphere alot dialogue Clone totally [P].\" We include results on the effect of the AUTOPROMPT hyperparameters in Appendix A. Accuracy in Low-Data Settings Although the goal of AUTOPROMPT is to probe a model\\'s knowl- edge, we also find that it may be a viable alternative to finetuning in the low-data regime. To show this, we measure the development set accuracy of AU- TOPROMPT prompts when using random subsets of 10, 100, and 1000 instances from the training data. We run our prompt search with |@trig| = 10, Vy| = 3, and | Vcand | = 10. We compare to the performance of BERT and RoBERTa finetuned on the same data. For fair comparison between AU- TOPROMPT and finetuning, we use Mosbach et al. (2020)\\'s recommended parameters for finetuning on small datasets: trained for 20 epochs, using AdamW (Loshchilov and Hutter, 2018) with bias correction and a learning rate that linearly increases to 2 × 10-5 in the first 10% of iterations, and lin- early decreases to 0 afterwards. Experiments are repeated 10 times on random subsets of data (and seeds for the finetuned models). Best-case, worst- case, and average performance are shown in Fig- ure 2. Note that results in the EMNLP version had a bug that has since been fixed. 0.9 Accuracy 0.7 - AutoPrompt 0.5 101 TT 102 103 (a) BERT on SST-2 0.9 Accuracy 0.7 0.5 101 102 103 (b) RoBERTa on SST-2 0.7 - Accuracy 0.5 - 0.3 101 102 103 (c) BERT on SICK-E 0.7 Accuracy 0.5 0.3 101 102 103 (d) RoBERTa on SICK-E Figure 2: Effect of Training Data on sentiment analy- sis and NLI for AUTOPROMPT vs. finetuning. X-axis is the number of data points used during training. Error bars plot the max. and min. accuracies observed over 10 independent runs. (revised since EMNLP version). We observe that while finetuning outperforms AUTOPROMPT on sentiment analysis, AUTO- PROMPT can perform better than finetuning on NLI. Notably, AUTOPROMPT elicits better average per- formance from both BERT and RoBERTa given only 10 training examples. Furthermore, results for RoBERTa are more stable across all sample sizes whereas finetuning can result in \"failed runs\" (con- sistent with Dodge et al. 2020). This behavior in the low-data regime is an interesting phenomenon, and suggests that there are barriers that MLMs must surmount when they are converted to fine- tuned classifiers that are not encountered when the task is presented as masked language modeling. 4 Natural Language Inference To evaluate the semantic understanding of MLMs, we experiment on Natural Language Inference\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Model Majority 56.7 33.3 BERT (linear probing) 68.0 49.5 91.9 RoBERTa (linear probing) 72.6 49.4 91.1 BERT (AUTOPROMPT) 62.3 65.0 69.3 87.3 SICK-E Datasets standard 3-way 2-way 50.0 BERT (finetuned) 86.7 84.0 95.6 55.4 85.7 RoBERTa (AUTOPROMPT) Table 2: Natural Language Inference performance on the SICK-E test set and variants. (Top) Baseline classi- fiers. (Bottom) Fill-in-the-blank MLMs. (NLI). NLI is crucial in many tasks such as reading comprehension and commonsense reasoning (Bow- man et al., 2015), and it is used as a common bench- mark for language understanding. Setup We use the entailment task from the SICK dataset (Marelli et al., 2014, SICK-E) which con- sists of around 10,000 pairs of human-annotated sentences labeled as entailment, contradiction, and neutral. The standard dataset is biased toward the neutral class which represent 56.7% of instances. We also experiment on an unbiased variant with 2-way classification of contradiction vs. entail- ment (2-way), as well as an unbiased 3-way clas- sification variant (3-way). The template used for AUTOPROMPT is provided in Table 3. We search over the following parameters: |Vcand| € {10, 50}, |Vy| E {1, 3, 5, 10}, | trig | € [1, 5], and choose the best prompt according to development set accuracy. Results Table 2 shows that AUTOPROMPT con- siderably outperforms the majority baseline in all experiments. For example, on the 2-way SICK-E dataset, AUTOPROMPT is comparable to a super- vised finetuned BERT. We also test linear probes- linear classifiers trained on top of frozen MLM representations with average pooling -and find AUTOPROMPT has comparable or higher accuracy, despite linear probes being susceptible to false pos- itives. Overall, these results demonstrate that both BERT and RoBERTa have some inherent knowl- edge of natural language inference. - We also examine the efficacy of AUTOPROMPT in the low-data regime (using the same procedure as SST-2) on the unbiased 3-way SICK-E data. The results in Figure 2 show that AUTOPROMPT per- forms on par with finetuned BERT and significantly better than finetuned RoBERTa in low data settings. MLMs Excel on Contradiction We find that the label tokens are more interpretable for con- tradiction compared to entailment or neutral (ex- amples in Table 3). We investigate if this hurts the model performance on entailment and neutral classes. We measure the precision for each la- bel in the 3-way balanced SICK-E dataset. BERT achieves 74.9%, 54.4%, and 36.8% precision for contradiction, entailment, and neutral cases, respec- tively, while RoBERTa obtains 84.9%, 65.1%, and 57.3%. These results suggest that AUTOPROMPT may be more accurate for concepts that can be eas- ily expressed using natural label tokens. 5 Fact Retrieval An important question is whether pretrained MLMs know facts about real-world entities. The LAMA dataset (Petroni et al., 2019) evaluates this using cloze tests that consist of (sub, rel, obj) triples, e.g. (Obama, bornIn, Hawaii), and manually created prompts with missing objects, e.g. \"Obama was born in [MASK].\". LPAQA (Jiang et al., 2020) ex- tends this idea by systematically creating prompts that are generated by mining Wikipedia, paraphras- ing, and crowdsourcing. In this section, we use the same cloze-style setup but automatically gener- ate prompts in order to better evaluate the factual knowledge of MLMs. We compare our approach against LAMA and LPAQA, which are explicitly designed for the task of fact retrieval. Setup We reformulate fact retrieval by mapping (sub,rel,obj) triples to a prompt using the template \"{sub} [T] ... [T][P].\", where the trigger tokens are specific to the relation rel and the correct object obj is the label token. We use the original test set from LAMA (Petroni et al., 2019), henceforth Orig- inal. To collect training data for AUTOPROMPT, we gather at most 1000 facts for each of the 41 re- lations in LAMA from the T-REX dataset (ElSahar et al., 2018). For the relations that still have less than 1000 samples, we gather extra facts straight from Wikidata. We ensure that none of the T-REX triples are present in the test set, and we split the data 80-20 into train and development sets. More- over, because the collected T-REX data is from a slightly different distribution than the LAMA test set, we also consider a separate evaluation where we split the T-REX triples into a 60-20-20 train/dev/test split and evaluate on the test set. This T-REx dataset is used to measure the performance of our prompts when the train and test data is from the same distribution.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Task Prompt Template Sentiment {sentence} [T] ... [T] [P]. Analysis NLI {prem } [P][T] .. . [T]{hyp} Fact Retrieval X plays Y music {sub} [T]. . . [T][P]. Relation X is a Y by profession Extraction { sent } { sub } [T]. . . [T][P]. Prompt found by AUTOPROMPT unflinchingly bleak and desperate Writing academicswhere overseas will appear [MASK]. Two dogs are wrestling and hugging [MASK] concretepathic workplace There is no dog wrestling and hugging Hall Overton fireplacemade antique son alto [MASK]. Leonard Wood (born February 4, 1942) is a former Canadian politician. Leonard Wood gymnasium brotherdicative himself another [MASK]. Label Tokens pos: partnership, extraordinary, ##bla neg: worse, persisted, unconstitutional con: Nobody, nobody, nor ent: ##found, ##ways, Agency neu: ##ponents, ##lary, ##uated Table 3: Example Prompts by AUTOPROMPT for each task. On the left, we show the prompt template, which combines the input, a number of trigger tokens [T], and a prediction token [P]. For classification tasks (sentiment analysis and NLI), we make predictions by summing the model\\'s probability for a number of automatically selected label tokens. For fact retrieval and relation extraction, we take the most likely token predicted by the model. We use AUTOPROMPT with 5 or 7 tokens, and select the search parameters using the T-REX de- velopment set. We prevent proper nouns and to- kens that appear as gold objects in the training data from being selected as trigger tokens. This is done to prevent AUTOPROMPT from \"cheating\" by em- bedding common answers inside the prompt. To evaluate, we observe the rank of the true object in label token distribution of the MLM, and use stan- dard ranking metrics: mean reciprocal rank (MRR), precision-at-1 (P@1), and precision-at-10 (P@10). Results Table 4 shows the performance of MLMs with different prompting methods, and we show qualitative examples in Table 3 and in Appendix C. Prompts generated using AUTOPROMPT can ex- tract factual knowledge from BERT more effec- tively than their manual and mined counterparts: we improve P@1 by up to 12 points. Moreover, despite AUTOPROMPT using only one prompt per relation, it still outperforms LPAQA\\'s ensemble method (which averages predictions for up to 30 prompts) by approximately 4 points. Using 7 trig- ger tokens achieves slightly higher scores than 5 trigger tokens, although the difference is not sub- stantial. This indicates that our approach is stable to the choice of trigger length, which is consis- tent with our sentiment analysis results. Overall, these results show that AUTOPROMPT can retrieve facts more effectively than past prompting meth- ods, thus demonstrating that BERT contains more factual knowledge than previously estimated. Relation Breakdown We also provide a detailed breakdown of the prompts found by Petroni et al. (2019) and AUTOPROMPT, and their associated ac- curacies in Appendix C, Table 7. Manual prompts are competitive when the prompt is easy to specify, e.g., the prompt \"was born in\" for the PLACE OF BIRTH relation. On the other hand, AUTOPROMPT performs especially well for relations that are dif- ficult to specify in a natural language prompt. For example, Petroni et al. (2019)\\'s prompt for the PO- SITION PLAYED ON TEAM relation is \"{sub} plays in [MASK] position\", which is not as specific as the relation requires. Although the prompt from AU- TOPROMPT is not grammatical (\"{sub} ediatric striker ice baseman defensive { obj} \"), it does con- tain tokens that are directly related to sports. BERT outperforms RoBERTa We finally di- rectly compare BERT and RoBERTa. To do so, we subsample the LAMA test set to consist of ex- amples where the object is a single token for both BERT and RoBERTa (Original-RoBERTa).4 BERT actually slightly outperforms RoBERTa, and we find that the prompts generated for RoBERTa tend to contain more irrelevant words (see Appendix C, Table 7). For example, the prompt generated by RoBERTa for the PLAYS INSTRUMENT relation contains words such as \"Trump\" and symbols such as \",\" (),\" for the POSITION PLAYED ON TEAM relation. It is surprising that RoBERTa does not 4The original dataset consists of examples where the object is a single token for BERT.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Prompt Type LAMA LPAQA (Top1) AUTOPROMPT 5 Tokens AUTOPROMPT 7 Tokens 53.89 MRR 40.27 43.57 53.06 Original P@10 P@1 59.49 31.10 35.79 54.29 26.38 62.03 34.10 72.17 42.94 73.93 43.34 54.89 72.02 45.57 MRR P@10 P@1 39.86 54.42 70.80 45.40 T-REX 57.27 31.16 Model BERT 55.22 RoBERTa 49.90 MRR P@10 74.01 68.34 40.01 P@1 45.23 Table 4: Factual Retrieval: On the left, we evaluate BERT on fact retrieval using the Original LAMA dataset from Petroni et al. (2019). For all three metrics (mean reciprocal rank, mean precision-at-10 (P@10), and mean precision-at-1(P@1)), AUTOPROMPT significantly outperforms past prompting methods. We also report results on a T-REx version of the data (see text for details). On the right, we compare BERT versus RoBERTa on a subset of the LAMA data using AUTOPROMPT with 5 tokens. perform better than BERT, and it is worthy of inves- tigating this further in future work. Additionally, recall that prompting is a lower bound on a model\\'s knowledge: the lower relative performance does not mean that the model actually knows less. 6 Relation Extraction Apart from evaluating whether MLMs know facts, it is also important to evaluate whether they can extract knowledge from text. In this section, we use the task of relation extraction (RE)-to identify how entities are related in a given sentence-an important task in information extraction. We create RE prompts in a similar fashion as fact retrieval: for a given triple (subj,rel,obj) and sentence that expresses this relation, we construct a prompt as \"{sent} {sub } [T]. .. [T][P].\", where the trigger to- kens are specific to the relation, and label token is the correct object obj (see Table 3 for an example). Setup We use the T-Rex dataset for RE because each T-REX fact comes with context sentences that mention the subject and object surface forms. We compare AUTOPROMPT to LAMA and LPAQA (their prompts are still useful here), as well as a re- cent supervised relation extraction model (Sorokin and Gurevych, 2017) that was also used by Petroni et al. (2019). To make the evaluation fair for the supervised RE model, we modify the standard RE evaluation. We give the model credit as long as it does not predict a different relation for the subject and object, i.e. we ignore the \"no relation\" pre- diction and all other relations. We also drop all sentences from evaluation for which the model\\'s named entity extractor failed to identify the sub- ject and the object as entities. See Appendix B for further details. For the evaluation of all systems, we treat a prediction as correct if it is either the canonical version of the object (e.g., \"USA\") or the rendered surface form (e.g., \"American\") for any of the context sentences in a given triple. Results T Table 5 shows the results for BERT and RoBERTa. MLMs can extract relational informa- tion more effectively than the supervised RE model, providing up to a 33% increase on the task when using AUTOPROMPT. RoBERTa also outperforms the supervised RE model, although it is worse than BERT (likely for similar reasons as we outline in Section 5). For both BERT and RoBERTa, we no- tice that the trigger tokens consist of words related to their corresponding relations (see Appendix D, Table 8 for full list), e.g. ROBERTa selects \"defy trademarks of namesake manufacturer\" for rela- tion MANUFACTURER/PRODUCER OF PRODUCT. Perturbed Sentence Evaluation A possible ex- planation for the strong results of MLMs in the RE setting is that they may already know many of the relations. Thus, they may directly predict the objects instead of extracting them. To separate this effect, we synthetically perturb the relation extrac- tion dataset by replacing each object in the test data with a random other object and making the same change to the prompt. For example, \"Ryo Kase (born November 9, 1974 in Yokohama >Yorkshire) is a Japanese actor\" where Ryo Kase is the subject, Yokohama is the original object, and Yorkshire is the new object. We regenerate the prompts using the perturbed version of the data. The accuracy of the RE model does not change significantly on the perturbed data (Table 5), how- ever, the accuracy of the MLMs decreases signifi- cantly. This indicates that a significant portion of MLM accuracy comes from background informa- tion rather than relation extraction. Nevertheless, our prompts for BERT outperform their LAMA and LPAQA counterparts, which provides further evi- dence that AUTOPROMPT produces better probes.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Model Supervised RE LSTM 57.95 58.81 BERT (LAMA) 69.06 28.02 BERT (LPAQA) 76.55 30.79 BERT (AUTOPROMPT) 90.73 56.43 RoBERTa (AUTOPROMPT) 60.33 28.95 Original Perturbed Table 5: Relation Extraction: We use prompts to test pretrained MLMs on relation extraction. Compared to a state-of-the-art LSTM model from 2017, MLMs have higher mean precision-at-1 (P@1), especially when us- ing prompts from AUTOPROMPT. We also test models on sentences that have been edited to contain incorrect facts. The accuracy of MLMs drops significantly on these sentences, indicating that their high performance stems from their factual knowledge. 7 Discussion Prompting as an Alternative to Finetuning The goal of prompting a language model is to probe the knowledge that the model acquired from pre- training. Nevertheless, prompting has some prac- tical advantages over finetuning for solving real- world tasks. First, as shown in Section 3, prompts generated using AUTOPROMPT can achieve higher accuracy than finetuning in the low-data regime. Moreover, prompting has advantages over finetun- ing when trying to solve many different tasks (e.g., the many users of the OpenAI GPT-3 API (Brown et al., 2020)). In particular, finetuning requires storing large language model checkpoints for each individual task, and drastically increases system cost and complexity because it requires deploying many different models at the same time. Prompt- ing alleviates both of these issues. Only prompts are stored for each individual task, while the same pretrained model is used across all of the tasks. Limitations of Prompting There are certain phenomena that are difficult to elicit from pre- trained language models via prompts. In our pre- liminary evaluation on datasets such as QQP (Iyer et al., 2017) and RTE (Dagan et al., 2005), prompts generated manually and with AUTOPROMPT did not perform considerably better than chance. How- ever, we cannot conclude that BERT does not know paraphrasing or entailment from these results. In general, different probing methods have different tasks and phenomena they are suitable for: AUTO- PROMPT makes prompt-based probes more gener- ally applicable, but, it still remains just one tool in the toolbox of the interpretability researcher. Limitations of AUTOPROMPT One downside of AUTOPROMPT is that it requires labeled train- ing data. Although this is also required for other probing techniques (e.g., linear probing classi- fiers), manual prompts rely on domain/language insights instead of labeled data. Compared to human-designed prompts, AUTOPROMPT gener- ated prompts lack interpretability, which is similar to other probing techniques, such as linear probing classifiers. Another limitation of AUTOPROMPT is that it can sometimes struggle when the training data is highly imbalanced. For example, in Sec- tions 4 and 5 we show that the prompts often just increase the likelihood of the majority label. Re- balancing the training data can help to mitigate this problem. Finally, due to the greedy search over the large discrete space of phrases, AUTOPROMPT is sometimes brittle; we leave more effective crafting techniques for future directions. 8 Conclusion In this paper, we introduce AUTOPROMPT, an approach to develop automatically-constructed prompts that elicit knowledge from pretrained MLMs for a variety of tasks. We show that these prompts outperform manual prompts while requir- ing less human effort. Furthermore, the results for sentiment analysis and textual entailment sug- gest that, in some data-scarce settings, it may be more effective to prompt language models than to finetune them for the task. Although we fo- cus only on masked language models in this paper, our method can be trivially extended to standard language models, and thus maybe useful for con- structing inputs for models like GPT-3 (Brown et al., 2020). Source code and datasets to re- produce the results in this paper is available at http://ucinlp.github.io/autoprompt. Acknowledgments We would like to thank the LAMA and LPAQA teams for answering our questions. We would also like to thank the members of UCI NLP, Matt Gard- ner, Sebastian Riedel, and Antoine Bosselut for valuable feedback. This material is based upon work sponsored by the DARPA MCS program un- der Contract No. N660011924033 with the United States Office Of Naval Research.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"References Zied Bouraoui, Jose Camacho-Collados, and Steven Schockaert. 2019. Inducing relational knowledge from BERT. In AAAI. Samuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large anno- tated corpus for learning natural language inference. In EMNLP. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv: 2005.14165. Alexis Conneau, Germán Kruszewski, Guillaume Lam- ple, Loïc Barrault, and Marco Baroni. 2018. What you can cram into a single vector: Probing sentence embeddings for linguistic properties. In ACL. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges Work- shop. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language under- standing. In NAACL. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stop- ping. arXiv preprint arXiv:2002.06305. Hady ElSahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare, Frédérique Laforest, and Elena Simperl. 2018. T-REX: A large scale alignment of natural language with knowledge base triples. In LREC. John Hewitt and Percy Liang. 2019. Designing and interpreting probes with control tasks. In EMNLP. Shankar Iyer, Nikhil Dandekar, and Kornel Csernai. 2017. First quora dataset release: Question pairs. Sarthak Jain and Byron C Wallace. 2019. Attention is not explanation. In NAACL. Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? In TACL. Sunjae Kwon, Cheongwoong Kang, Jiyeon Han, and Jaesik Choi. 2019. Why do masked neural language models still need common sense knowledge? arXiv preprint arXiv: 1911.03024. Patrick Lewis, Ludovic Denoyer, and Sebastian Riedel. 2019. Unsupervised question answering by cloze translation. In ACL. Nelson F Liu, Matt Gardner, Yonatan Belinkov, Matthew Peters, and Noah A Smith. 2019. Linguis- tic knowledge and transferability of contextual rep- resentations. In NAACL. Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Con- ference on Learning Representations. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, Roberto Zamparelli, et al. 2014. A SICK cure for the evaluation of com- positional distributional semantic models. In LREC. Marius Mosbach, Maksym Andriushchenko, and Diet- rich Klakow. 2020. On the stability of fine-tuning bert: Misconceptions, explanations, and strong base- lines. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In NAACL. Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Se- bastian Riedel. 2019. Language models as knowl- edge bases? In EMNLP. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Techni- cal report. Timo Schick and Hinrich Schütze. 2020. Exploit- ing cloze questions for few-shot text classification and natural language inference. arXiv preprint arXiv:2001.07676. Vered Shwartz, Peter West, Ronan Le Bras, Chan- dra Bhagavatula, and Yejin Choi. 2020. Unsuper- vised commonsense question answering with self- talk. arXiv preprint arXiv: 2004.05483. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In EMNLP. Daniil Sorokin and Iryna Gurevych. 2017. Context- aware representations for knowledge base relation extraction. In EMNLP. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv: 1806.02847. Elena Voita and Ivan Titov. 2020. Information- theoretic probing with minimum description length. In EMNLP. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial trig- gers for attacking and analyzing NLP. In EMNLP.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2019. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In ICLR. Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In EMNLP. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Fun- towicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Can- wen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2019. HuggingFace\\'s Transformers: State-of-the- art natural language processing. arXiv preprint arXiv: 1910.03771.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"A Effect of Hyperparameters on Sentiment Analysis BERT 0.90 0.85 - Accuracy 0.80 0.75 JCtrig| =3 ICtrig | = 4 @trig |=5 CCtrig |=6 RoBERTa 0.90 0.85 - Accuracy 0.80 0.75 1 2 3 4 5 Label Set Size, |Vy| Figure 3: Effect of Label and Trigger Set Sizes on sentiment analysis. The number of candidate replace- ments is fixed at | Vcand| = 100. Increasing the label set size improves performance, while changing the trigger length does not have much impact. To measure the effects of the AUTOPROMPT search hyperparameters, we plot the validation ac- curacy as a function of label set size | Vy| and the number of trigger tokens |@trig| in Figure 3. We fix the number of candidates at | Vcand| = 100. We observe similar trends when | Vcand| = 10. Varying the number of trigger tokens generally has little effect. On the other hand, there is a sub- stantial increase in accuracy when increasing the label set size from 1 to 3 (approximately +5% for BERT, and +10% for RoBERTa). After analyzing the label sets, we find that our method generally produces intuitive results-\"marvelous\" and \"phi- lanthrop\" are associated with positive sentiment, whereas \"worse\" and \"incompetence\" are associ- ated with negative sentiment for RoBERTa. B Relation Extraction Details Following Petroni et al. (2019), we use the pre- trained RE model from Sorokin and Gurevych (2017) as our baseline. To encode the sentence, this model uses a combination of an LSTM-based relation encoder and an attention mechanism. To make predictions, the model constructs a knowl- edge graph whose edges are the extracted relation triples. The standard RE evaluation measures how well the model predicts the relation types of entity pairs on the sentence level. Since our goal is to extract the object of relation triplets, rather than the relation itself, we tweak the standard RE evaluation. We feed the RE model sentences from test facts and we query the resulting graph for all edges that contain the given subject and relation. Then we select the triple with the highest confidence and compare it\\'s object to the gold object. We do this for every fact and take the average across all relations to get the overall preci- sion. The RE model is not trained to predict two of the original T-REX relations. For fair comparison, we exclude these two relations for our evaluation.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"C Additional Fact Retrieval Results Relation Manual Prompt (LAMA) P1001 P101 [X] works in the field of [Y] 864 9.91 P103 The native language of [X] is [Y] P106 [X] is a [Y] by profession P108 [X] works for [Y] 5.74 P127 [X] is owned by [Y] 32.46 P1303 P131 [X] is located in [Y] 1000 23.27 P136 [X] plays [Y] music 1000 0.75 P1376 [X] is the capital of [Y] 310 73.93 P138 [X] is named after [Y] 856 61.55 59.69 P140 [X] is affiliated with the [Y] religion 445 0.63 59.83 P1412 [X] used to communicate in [Y] 1000 65.02 P159 P17 P176 P178 [X] is developed by [Y] 560 62.84 59.12 P19 [X] was born in [Y] 1000 21.08 20.87 P190 [X] and [Y] are twin cities 895 P20 P264 [X] is represented by music label [Y] 1000 9.56 10.26 43.82 P27 [X] is [Y] citizen 1000 0.0 P276 [X] is located in [Y] 1000 41.5 41.5 44.11 P279 [X] is a subclass of [Y] 1000 30.74 14.75 54.93 P30 [X] is located in [Y] P31 [X] is a [Y] 36.66 P36 The capital of [X] is [Y] 1000 62.16 62.16 60.6 P361 [X] is part of [Y] P364 The original language of [X] is [Y] 1000 P37 The official language of [X] is [Y] 311 56.83 62.63 P39 [X] has the position of [Y] 7.96 P407 [X] was written in [Y] 68.42 P413 [X] plays in [Y] position P449 [X] was originally aired on [Y] P463 [X] is a member of [Y] 679 P47 [X] shares border with [Y] 1000 P495 P527 [X] consists of [Y] P530 [X] maintains diplomatic relations with [Y] 927 P740 [X] was founded in [Y] 1000 P937 [X] used to work in [Y] 39.1 [X] is a legal term in [Y] 1000 [X] plays [Y] 1000 7.59 The headquarter of [X] is in [Y] 1000 32.37 35.57 35.47 [X] is located in [Y] 1000 31.29 [X] is produced by [Y] 1000 85.64 81.67 [X] died in [Y] [X] was created in [Y] 1000 #train 1000 72.16 1000 0.63 376 6.79 548 34.79 1000 27.91 27.91 31.16 1000 25.44 18.56 70.36 1000 36.66 51.95 1000 23.61 31.44 17.7 1000 16.14 30.72 1000 59.18 65.22 1000 0.53 1000 20.89 9.08 34.39 1000 1000 29.77 LAMA LPAQA 70.47 2.41 44.51 43.93 48.48 54.55 67.11 57.33 54.22 13.67 13.34 19.52 16.5 32.23 36.63 11.07 10.55 25.61 2.81 3.92 3.11 7.59 AUTOPROMPT 72.75 5.32 12.79 72.16 0.0 18.02 22.81 16.76 59.83 64.71 35.48 52.15 1.91 41.51 46.69 23.74 41.7 13.68 13.89 82.45 82.09 14.72 8.62 35.95 15.38 37.46 55.42 40.17 66.05 75.26 71.21 87.78 66.72 19.92 2.31 38.36 Table 6: A breakdown of all relations for fact retrieval on the original dataset from Petroni et al. (2019). We compare P@1 of prompts generated by LAMA, LPAQA, and our approach using five prompt tokens.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"Relation Method P101 Manual AUTOPROMPT BERT AUTOPROMPT RoBERTa P103 Manual AUTOPROMPT BERT P106 Manual [X] is a [Y] by profession 0.73 P127 Manual [X] is owned by [Y] 36.67 AUTOPROMPT BERT X] is hindwings mainline architecture within [Y] 47.01 AUTOPROMPT RoBERTa [X] picThom unwillingness officially governs [Y] 39.58 P1303 Manual [X] plays [Y] 18.91 AUTOPROMPT BERT AUTOPROMPT RoBERTa [X] playingdrum concertoative electric [Y] 42.69 [X]Trump learned soloKeefe classical [Y] 44.44 P136 Manual [X] plays [Y] music 0.7 AUTOPROMPT BERT [X] freaking genre orchestra fiction acid [Y] 59.95 AUTOPROMPT RoBERTa [X] blends postwar hostage drama sax [Y] 52.97 P1376 Manual [X] is the capital of [Y] 81.11 AUTOPROMPT BERT AUTOPROMPT RoBERTa P178 Manual [X] is developed by [Y] 62.76 AUTOPROMPT BERT AUTOPROMPT RoBERTa X] is memory arcade branding by [Y] 64.45 [X] 1987 floppy simulator users sued [Y] 69.56 P20 Manual [X] died in [Y] 32.07 AUTOPROMPT BERT X] reorganizationotype photographic studio in [Y] 33.53 AUTOPROMPT RoBERTa [X] .. enigmatic twentieth nowadays near [Y] 31.33 P27 Manual [X] is [Y] citizen 0.0 AUTOPROMPT BERT AUTOPROMPT RoBERTa P276 Manual [X] is located in [Y] 43.73 AUTOPROMPT BERT AUTOPROMPT RoBERTa P279 Manual [X] is a subclass of [Y] 31.04 AUTOPROMPT BERT AUTOPROMPT RoBERTa [X] is î adequately termed coated [Y] 55.65 [X], formerly prayers unstaceous [Y] 52.55 P37 Manual AUTOPROMPT BERT AUTOPROMPT RoBERTa P407 Manual AUTOPROMPT BERT AUTOPROMPT RoBERTa [X] was written in [Y] 60.21 [X] playedić every dialect but [Y] 69.31 [X] scaven pronunciation. * Wikipedia speaks [Y] 72.0 P413 AUTOPROMPT RoBERTa AUTOPROMPT BERT [X] supporters studied politicians musician turned [Y] 15.83 AUTOPROMPT RoBERTa [X] (), astronomers businessman.former [Y] 19.24 Manual AUTOPROMPT RoBERTa Prompt P@1 [X] works in the field of [Y] 11.52 [X] probability earliest fame totaled studying [Y] 15.01 [X] 1830 dissertation applying mathsucci [Y] 0.17 The native language of [X] is [Y] 74.54 [X]PA communerug speaks proper [Y] 84.87 [X]neau optionally fluent! ? \\'traditional [Y] 81.61 X] boasts native territory traditionally called [Y] 63.33 [X] limestone depositedati boroughDepending [Y] 28.33 [X] m3 badminton pieces internationally representing [Y] 46.13 [X] offic organise forests statutes northwestern [Y] 42.07 [X] consists kilograms centred neighborhoods in [Y] 44.64 [X] manoeuv constructs whistleblowers hills near [Y] 37.47 The official language of [X] is [Y] 56.89 [X]inen dialects resembled officially exclusively [Y] 54.44 [X]onen tribes descending speak mainly [Y] 53.67 [X] plays in [Y] position 0.53 AUTOPROMPT BERT [X] played colors skier <> defensive [Y] 41.71 [X],\" (), ex-, Liverpool [Y] 23.21 Table 7: Examples of manual prompts (first line, shown with BERT\\'s P@1) and prompts generated via AUTO- PROMPT for Fact Retrieval.\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"D Additional Relation Extraction Results Relation P103 (native language) P36 (capital) P530 (diplomatic relation) BERT P106 (occupation) RoBERTa Spencer Treat Clark (born September 24, 1987) is an Amer- Hulk ican actor who has appeared in several films, including Glad- iator, Mystic River, and Unbreakable. Spencer Treat Clark famously the famously handsome the [MASK]. P276 (location) P176 (manufacturer) RoBERTa P279 (subclass of) BERT Mizeria is a Polish saladsandwich consisting of thinly sliced or grated cucumbers, often with sour cream though in some cases oil. Mizeria is calls direcend altitude [MASK]. P463 (member of) Model BERT RoBERTa Kirk was born in Clinton County, Ohio, and he entered ser- Wilmington vice in Wilmington, Ohio. Clinton County famously includes the zoo influencing [MASK]. BERT RoBERTa Context and Prompt Prediction Alexandra Lamy (born 14 October 1971) is a French actress. Alexandra Lamy speaks airfield dripping % of [MASK]. French The Black Sea forms in an east-west trending elliptical de- pression which lies between Bulgaria, Georgia, Romania, Russia, Turkey, and Ukraine. Ukraine qualified some im- migration actually entered [MASK]. Russia The Immortal Game was a chess game played by Adolf Anderssen and Lionel Kieseritzky on 21 June 1851 in Len- denSeoul, during a break of the first international tourna- ment. The Immortal Game locatedstered regardless streets in [MASK]. The Honda Civic del Sol is a 2-seater front-engined, front wheel drive, targa top car manufactured by Honda Toyota in the 1990s. Honda Civic del Sol defy trademarks of name- sake manufacturer [MASK]. Seoul Toyota food RushAerosmith was a Canadian rock band consisting of Kiss Geddy Lee (bass, vocals, keyboards), Alex Lifeson (guitars), and Neil Peart (drums, percussion, lyricist). Alex Lifeson affiliatedalach the internationally initials [MASK]. Table 8: Examples of prompts generated using AUTOPROMPT for relation extraction. Underlined words represent the gold object. The bottom half of the Table shows examples of our augmented evaluation where the original objects (represented by crossed-out words) are replaced by new objects.\"\\n\\n\\nCONCISE SUMMARY:']"
     ]
    }
   ],
   "source": [
    "doc_sum = summary_eng.generate_summary_files(source_folder=EXTRACTED_DATA_FOLDER, destination_folder=SUMMARY_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
